<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Properties of Estimators and Hypothesis Testing | Data Analysis and Statistical Thinking: An R Workbook</title>
  <meta name="description" content="This is a guide for the Globe Data Science Course." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Properties of Estimators and Hypothesis Testing | Data Analysis and Statistical Thinking: An R Workbook" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a guide for the Globe Data Science Course." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Properties of Estimators and Hypothesis Testing | Data Analysis and Statistical Thinking: An R Workbook" />
  
  <meta name="twitter:description" content="This is a guide for the Globe Data Science Course." />
  

<meta name="author" content="Fernando Racimo, Shyam Gopalakrishnan" />


<meta name="date" content="2021-04-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-models.html"/>
<link rel="next" href="likelihood-based-inference.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis and Statistical Thinking</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Getting Started with Data Analysis</a></li>
<li class="chapter" data-level="3" data-path="prob1.html"><a href="prob1.html"><i class="fa fa-check"></i><b>3</b> Probability Part 1</a>
<ul>
<li class="chapter" data-level="3.1" data-path="prob1.html"><a href="prob1.html#todays-programme"><i class="fa fa-check"></i><b>3.1</b> Today’s programme</a></li>
<li class="chapter" data-level="3.2" data-path="prob1.html"><a href="prob1.html#the-bernoulli-distribution-tossing-a-coin"><i class="fa fa-check"></i><b>3.2</b> The Bernoulli distribution: tossing a coin</a></li>
<li class="chapter" data-level="3.3" data-path="prob1.html"><a href="prob1.html#adding-up-coin-tosses"><i class="fa fa-check"></i><b>3.3</b> Adding up coin tosses</a></li>
<li class="chapter" data-level="3.4" data-path="prob1.html"><a href="prob1.html#the-expectation"><i class="fa fa-check"></i><b>3.4</b> The expectation</a></li>
<li class="chapter" data-level="3.5" data-path="prob1.html"><a href="prob1.html#our-first-probability-mass-function"><i class="fa fa-check"></i><b>3.5</b> Our first probability mass function</a></li>
<li class="chapter" data-level="3.6" data-path="prob1.html"><a href="prob1.html#the-variance"><i class="fa fa-check"></i><b>3.6</b> The variance</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability-part-2.html"><a href="probability-part-2.html"><i class="fa fa-check"></i><b>4</b> Probability Part 2</a></li>
<li class="chapter" data-level="5" data-path="probability-catch-up.html"><a href="probability-catch-up.html"><i class="fa fa-check"></i><b>5</b> Probability Catch-up</a>
<ul>
<li class="chapter" data-level="5.1" data-path="probability-catch-up.html"><a href="probability-catch-up.html#discrete-distributions"><i class="fa fa-check"></i><b>5.1</b> Discrete distributions</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="probability-catch-up.html"><a href="probability-catch-up.html#poisson-distribution"><i class="fa fa-check"></i><b>5.1.1</b> Poisson distribution</a></li>
<li class="chapter" data-level="5.1.2" data-path="probability-catch-up.html"><a href="probability-catch-up.html#geometric-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Geometric distribution</a></li>
<li class="chapter" data-level="5.1.3" data-path="probability-catch-up.html"><a href="probability-catch-up.html#sampling-from-arbitrary-distributions"><i class="fa fa-check"></i><b>5.1.3</b> Sampling from arbitrary distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="probability-catch-up.html"><a href="probability-catch-up.html#the-normal-distribution-and-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2</b> The Normal distribution and the Central Limit Theorem</a></li>
<li class="chapter" data-level="5.3" data-path="probability-catch-up.html"><a href="probability-catch-up.html#the-exponential-distribution"><i class="fa fa-check"></i><b>5.3</b> The exponential distribution</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>6</b> Linear Models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-models.html"><a href="linear-models.html#fitting-a-simple-linear-regression"><i class="fa fa-check"></i><b>6.1</b> Fitting a simple linear regression</a></li>
<li class="chapter" data-level="6.2" data-path="linear-models.html"><a href="linear-models.html#interpreting-a-simple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Interpreting a simple linear regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-models.html"><a href="linear-models.html#simulating-data-from-a-linear-model"><i class="fa fa-check"></i><b>6.3</b> Simulating data from a linear model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="properties-of-estimators-and-hypothesis-testing.html"><a href="properties-of-estimators-and-hypothesis-testing.html"><i class="fa fa-check"></i><b>7</b> Properties of Estimators and Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="properties-of-estimators-and-hypothesis-testing.html"><a href="properties-of-estimators-and-hypothesis-testing.html#properties-of-point-estimators"><i class="fa fa-check"></i><b>7.1</b> Properties of point estimators</a></li>
<li class="chapter" data-level="7.2" data-path="properties-of-estimators-and-hypothesis-testing.html"><a href="properties-of-estimators-and-hypothesis-testing.html#obtaining-confidence-intervals"><i class="fa fa-check"></i><b>7.2</b> Obtaining confidence intervals</a></li>
<li class="chapter" data-level="7.3" data-path="properties-of-estimators-and-hypothesis-testing.html"><a href="properties-of-estimators-and-hypothesis-testing.html#hypothesis-testing"><i class="fa fa-check"></i><b>7.3</b> Hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="likelihood-based-inference.html"><a href="likelihood-based-inference.html"><i class="fa fa-check"></i><b>8</b> Likelihood-based inference</a>
<ul>
<li class="chapter" data-level="8.1" data-path="likelihood-based-inference.html"><a href="likelihood-based-inference.html#meteorite-data"><i class="fa fa-check"></i><b>8.1</b> Meteorite data</a></li>
<li class="chapter" data-level="8.2" data-path="likelihood-based-inference.html"><a href="likelihood-based-inference.html#simple-linear-regression"><i class="fa fa-check"></i><b>8.2</b> Simple linear regression</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>9</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="9.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#using-bayes-rule-covid-19"><i class="fa fa-check"></i><b>9.1</b> Using Bayes’ rule: Covid-19</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#first-foray-into-bayesian-inference"><i class="fa fa-check"></i><b>9.2</b> First foray into Bayesian inference</a></li>
<li class="chapter" data-level="9.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#conjugate-prior-rejection-sampling-and-mcmc"><i class="fa fa-check"></i><b>9.3</b> Conjugate prior, Rejection sampling and MCMC*</a></li>
<li class="chapter" data-level="9.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayesian-point-estimates-and-credible-intervals"><i class="fa fa-check"></i><b>9.4</b> Bayesian point estimates and credible intervals</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#point-estimates"><i class="fa fa-check"></i><b>9.4.1</b> Point estimates</a></li>
<li class="chapter" data-level="9.4.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals"><i class="fa fa-check"></i><b>9.4.2</b> Credible intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>10</b> Classification</a></li>
<li class="chapter" data-level="11" data-path="model-assessment.html"><a href="model-assessment.html"><i class="fa fa-check"></i><b>11</b> Model Assessment</a></li>
<li class="chapter" data-level="12" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>12</b> Resampling</a>
<ul>
<li class="chapter" data-level="12.1" data-path="resampling.html"><a href="resampling.html#the-bootstrap"><i class="fa fa-check"></i><b>12.1</b> The bootstrap</a></li>
<li class="chapter" data-level="12.2" data-path="resampling.html"><a href="resampling.html#permutation-test"><i class="fa fa-check"></i><b>12.2</b> Permutation test</a></li>
<li class="chapter" data-level="12.3" data-path="resampling.html"><a href="resampling.html#validation"><i class="fa fa-check"></i><b>12.3</b> Validation</a></li>
<li class="chapter" data-level="12.4" data-path="resampling.html"><a href="resampling.html#cross-validation"><i class="fa fa-check"></i><b>12.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>13</b> Mixed Models</a></li>
<li class="chapter" data-level="14" data-path="ordination.html"><a href="ordination.html"><i class="fa fa-check"></i><b>14</b> Ordination</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ordination.html"><a href="ordination.html#libraries-and-data"><i class="fa fa-check"></i><b>14.1</b> Libraries and Data</a></li>
<li class="chapter" data-level="14.2" data-path="ordination.html"><a href="ordination.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>14.2</b> Principal component analysis (PCA)</a></li>
<li class="chapter" data-level="14.3" data-path="ordination.html"><a href="ordination.html#pca-under-the-hood"><i class="fa fa-check"></i><b>14.3</b> PCA under the hood</a></li>
<li class="chapter" data-level="14.4" data-path="ordination.html"><a href="ordination.html#principal-components-as-explanatory-variables"><i class="fa fa-check"></i><b>14.4</b> Principal components as explanatory variables</a></li>
<li class="chapter" data-level="14.5" data-path="ordination.html"><a href="ordination.html#nmds"><i class="fa fa-check"></i><b>14.5</b> NMDS</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>15</b> Clustering</a>
<ul>
<li class="chapter" data-level="15.1" data-path="clustering.html"><a href="clustering.html#libraries-and-data-1"><i class="fa fa-check"></i><b>15.1</b> Libraries and Data</a></li>
<li class="chapter" data-level="15.2" data-path="clustering.html"><a href="clustering.html#distances"><i class="fa fa-check"></i><b>15.2</b> Distances</a></li>
<li class="chapter" data-level="15.3" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>15.3</b> K-means clustering</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html"><i class="fa fa-check"></i><b>16</b> REcoStats: Linear Models</a>
<ul>
<li class="chapter" data-level="16.1" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html#fitting-a-simple-linear-regression-1"><i class="fa fa-check"></i><b>16.1</b> Fitting a simple linear regression</a></li>
<li class="chapter" data-level="16.2" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html#interpreting-a-simple-linear-regression-1"><i class="fa fa-check"></i><b>16.2</b> Interpreting a simple linear regression</a></li>
<li class="chapter" data-level="16.3" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html#simulating-data-from-a-linear-model-1"><i class="fa fa-check"></i><b>16.3</b> Simulating data from a linear model</a></li>
<li class="chapter" data-level="16.4" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html#hypothesis-testing-and-permutation-testing"><i class="fa fa-check"></i><b>16.4</b> Hypothesis testing and permutation testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis and Statistical Thinking: An R Workbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="properties-of-estimators-and-hypothesis-testing" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Properties of Estimators and Hypothesis Testing</h1>
<div id="properties-of-point-estimators" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Properties of point estimators</h2>
<p>In this exercise, we’ll draw many simulated samples from a known distribution with known parameters. We will then consider these as instances of real datasets, and estimate parameters of the original distributions using different estimators applied to the datasets. Our goal will be to analyze properties of the estimators, namely their sampling distribution, bias and variance.</p>
<p>Recall the definitions of two important properties of point estimators:</p>
<p>Bias: <span class="math display">\[B(\hat\theta_n) =  E[\hat\theta_n(D) - \theta] \]</span></p>
<p>The bias reflects the average difference between our estimator and the true parameter.</p>
<p>Variance: <span class="math display">\[Var(\hat\theta_n) =  E[ (\hat\theta_n(D) - E[\hat\theta_n(D)] )^2] = SE(\hat\theta_n)^2\]</span></p>
<p>The variance reflects the variation of the estimator’s sampling distribution around its own mean (regardless of what the true parameter is). It is also the square of the standard error.</p>
<blockquote>
<p>In real life, we will generally not know the sampling distribution of our test statistic, but we can attempt to approximate it. As we’ll see below, simulations can be of great help here.</p>
</blockquote>
<p>We will first create a function that draws a user-specified number (<code>nsim</code>) of independent data samples of size <code>sampsize</code> from a distribution of choice. By default this distribution is the Normal distribution (<code>rnorm</code>), but it can be changed by the user:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="properties-of-estimators-and-hypothesis-testing.html#cb66-1" aria-hidden="true" tabindex="-1"></a>simsamps <span class="ot">&lt;-</span> <span class="cf">function</span>(sampsize, <span class="at">nsim =</span> <span class="dv">10000</span>, <span class="at">rsim =</span> rnorm, ...){</span>
<span id="cb66-2"><a href="properties-of-estimators-and-hypothesis-testing.html#cb66-2" aria-hidden="true" tabindex="-1"></a>  sims <span class="ot">&lt;-</span> <span class="fu">rsim</span>(sampsize<span class="sc">*</span>nsim,...) <span class="co"># generate draws from a specified distribution</span></span>
<span id="cb66-3"><a href="properties-of-estimators-and-hypothesis-testing.html#cb66-3" aria-hidden="true" tabindex="-1"></a>  simmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">sample</span>(sims), <span class="at">nrow=</span>nsim, <span class="at">ncol=</span>sampsize) <span class="co"># organize those draws into a matrix</span></span>
<span id="cb66-4"><a href="properties-of-estimators-and-hypothesis-testing.html#cb66-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(simmat) <span class="co"># provide the matrix as output</span></span>
<span id="cb66-5"><a href="properties-of-estimators-and-hypothesis-testing.html#cb66-5" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The output of this function is a matrix, with row number equal to the number of generated simulations, and column number equal to the size of each simulation. For example, to create 10,000 simulations, each of size 30, from a normal distribution, we would write:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="properties-of-estimators-and-hypothesis-testing.html#cb67-1" aria-hidden="true" tabindex="-1"></a>testmat <span class="ot">&lt;-</span> <span class="fu">simsamps</span>(<span class="dv">30</span>,<span class="dv">10000</span>,rnorm) <span class="co"># generate matrix of 10,000 Normal data sets, each of size 30</span></span>
<span id="cb67-2"><a href="properties-of-estimators-and-hypothesis-testing.html#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(testmat) <span class="co"># dimensions of matrix</span></span></code></pre></div>
<pre><code>## [1] 10000    30</code></pre>
<p>What type of Normal distribution are we sampling from though? By default, the function rnorm samples from a standard Normal distribution with mean equal to 0 and standard deviation equal to 1. In our function <code>simsamps</code>, we are only feeding one argument to the normal distribution (the number of draws we want to obtain from it). The three dots placed in both the argument of the <code>simsamps</code> function and in its internal call to <code>rsim</code> allows us to feed more parameters to <code>rsim</code>:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="properties-of-estimators-and-hypothesis-testing.html#cb69-1" aria-hidden="true" tabindex="-1"></a>testmat <span class="ot">&lt;-</span> <span class="fu">simsamps</span>(<span class="dv">30</span>,<span class="dv">100</span>,rnorm,<span class="at">mean=</span><span class="dv">3</span>,<span class="at">sd=</span><span class="dv">5</span>) <span class="co"># generate matrix of 1000 Normal(3,5) data sets of size 30</span></span></code></pre></div>
<p>We can obtain the sample mean and sample median from each of these datasets, using the <code>apply</code> function:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="properties-of-estimators-and-hypothesis-testing.html#cb70-1" aria-hidden="true" tabindex="-1"></a>testmean <span class="ot">&lt;-</span> <span class="fu">apply</span>(testmat,<span class="dv">1</span>,mean)</span>
<span id="cb70-2"><a href="properties-of-estimators-and-hypothesis-testing.html#cb70-2" aria-hidden="true" tabindex="-1"></a>testmedian <span class="ot">&lt;-</span> <span class="fu">apply</span>(testmat,<span class="dv">1</span>,median)</span></code></pre></div>
<p><strong>Exercise</strong>: Create 20,000 simulated data sets of size 100, each drawn from a Normal distribution with expected value equal to 4 and standard deviation equal to 10. Create a histogram of the sample means from all the data sets. Draw a blue line where the average of all these means is located, and a red line where the <strong>expected value</strong> of the original source distribution is located. Next, do the same for all the sample medians.</p>
<p>Now, try answering these questions: Is the sample mean a biased estimator of the expected value of a normal distribution? Is the sample median a biased estimator of the expected value of a normal distribution? Which estimator has the highest variance? Which estimator would you use if given a dataset like one of the ones we simulated, in order to estimate the expected value of this Normal distribution?</p>
<p><strong>Exercise</strong>: Create 20,000 simulated data sets of size 100, each drawn from an Exponential distribution with rate equal to 2. Create a histogram of the sample means from all the data sets. Draw a blue line where the average of all these means is located, and a red line where the <strong>expected value</strong> of the original source distribution is located. Recall that the expected value of this distribution is 1/rate = 0.5.</p>
<p>Now, try answering these questions: Is the sample mean a biased estimator of the expected value of an exponential distribution? Is the sample median a biased estimator of the expected value of an exponential distribution? Which of these two estimators would you use if you were trying to estimate the rate of an exponential process (e.g. the rate at which buses arrive at a station), from a dataset like one of the ones we just simulated?</p>
<p>The histograms we’ve built reflect the <strong>sampling distributions of estimators</strong>. They serve to visualize the spread of an estimator’s distribution around its own mean, and allow us to determine whether that estimator’s mean is equal to the expected value of our distribution of interest.</p>
<blockquote>
<p>The ‘sampling distribution’ is the distribution of a particular test statistic, like the sample mean or the sample median. This is different from the distribution of the data itself. For example, a data set may come from a particular distribution, say Poisson, Normal or Exponential. The sample mean of such a dataset is a single number. If we had different data sets of equal size, we would be able to obtain different sample means. The distribution of those means (in the limit of infinite datasets) is the sampling distribution, which will often be Normal, if the conditions of the Central Limit Theorem apply. For other statistics, for example, the sample median, the Central Limit Theorem might not apply.</p>
</blockquote>
</div>
<div id="obtaining-confidence-intervals" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Obtaining confidence intervals</h2>
<p>Confidence intervals (CIs) denote how sure we are about the value of a parameter. Importantly, CIs are statements made about an infinite number of datasets. For example, let’s imagine a parameter of interest called <span class="math inline">\(\theta\)</span>, which has a value that we don’t know. If we had an infinite number of data sets (<span class="math inline">\(i=1,2,3,...,\infty\)</span>), the lower and upper 95% confidence intervals for each dataset <span class="math inline">\(i\)</span> are the two values that would bound (contain) the unknown parameter <span class="math inline">\(\theta\)</span> in 95% of those datasets.</p>
<p>Generally, we only have <strong>one set of data points</strong>, so this statement might sound a bit confusing. How can we make statements about infinite data sets, when we only have one set? In practice, the confidence interval is an approximation based on the <strong>spread (variance) of an estimator’s (<span class="math inline">\(\hat{\theta}\)</span>) sampling distribution around the expected value of the unknown parameter (<span class="math inline">\(\theta\)</span>)</strong>.</p>
<p>If a given estimator <span class="math inline">\(\hat{\theta}\)</span> of a data set of size n:</p>
<ol style="list-style-type: decimal">
<li>has a known standard error:</li>
</ol>
<p><span class="math display">\[SE(\hat{\theta}_n) = \sqrt{Var(\hat{\theta}_n)}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>is unbiased:</li>
</ol>
<p><span class="math display">\[E[\hat{\theta}_n - \theta] = 0\]</span></p>
<p>and</p>
<ol start="3" style="list-style-type: decimal">
<li>has a normal sampling distribution:</li>
</ol>
<p><span class="math display">\[\hat{\theta}_n \sim Normal\]</span></p>
<p>then the confidence intervals that will contain the true value of the parameter 95% of the time are:</p>
<p><span class="math display">\[(\hat{\theta} + SE(\hat{\theta}_n) q_{2.5\%}, \ \hat{\theta} + SE(\hat{\theta}_n) q_{97.5\%})\]</span>
Here, <span class="math inline">\(q_{x\%}\)</span> is the <span class="math inline">\(x\%\)</span> quantile function of a standard Normal(0,1) distribution. This value marks a limit such that <span class="math inline">\(x\%\)</span> of the probability mass of a distribution is to the left (lower than the value), and <span class="math inline">\(1-x\%\)</span> is to the right (higher than the value).</p>
<p>Because the standard normal(0,1) distribution is symmetric and centered around 0, <span class="math inline">\(q_{2.5\%} = -q_{97.5\%}\)</span>, so we can also write the CI as:</p>
<p><span class="math display">\[(\hat{\theta} - SE(\hat{\theta}_n)  q_{97.5\%}, \
 \hat{\theta} + SE(\hat{\theta}_n) q_{97.5\%})\]</span></p>
<p>If we compute these boundaries for a particular data set we study, and the estimator we’re applying satisfies the 3 conditions above, we can be sure that these boundaries will contain the true parameter <span class="math inline">\(95\%\)</span> of the time (out of an infinite number of possible data sets that we could have obtained, in theory, from the phenomenon of interest).</p>
<blockquote>
<p>Note that the sampling distribution of the mean will tend to be normal and unbiased for large sample sizes, because of the Central Limit Theorem! However, the sampling distribution of other statistics (like the median) may be neither normal nor unbiased. For example, as we saw in the previous section, the sampling distribution of the median of a data set that is exponentially distributed is biased! Thus, it would be inappropriate to compute the 95% CIs using the equation above in that case. Later on in this class, we’ll figure out ways to obtain more general CIs that do not depend on the strict assumptions of unbiasedness and normality of the estimator’s sampling distribution.</p>
</blockquote>
<p>The mean of a normally distributed dataset is both unbiased and normally distributed, so the stated assumptions hold in that case. Let’s verify that:</p>
<p><strong>Exercise</strong>: Create 20,000 simulated data sets of size 5, each drawn from a Normal distribution with expected value equal to 4 and standard deviation equal to 10. For each dataset, compute the sample mean. Then, compute the standard error of the mean (standard deviation over all means). Finally, use the standard error to compute the <span class="math inline">\(95\%\)</span> confidence intervals for each data set, using the formula stated above. How often do these confidence intervals contain the expected value? Hint: to obtain the quantiles of a Normal distribution, you can use the function <code>qnorm</code>. For example, the <span class="math inline">\(35\%\)</span> quantile of a standard Normal(0,1) distribution is equal to <code>qnorm(0.35,mean=0,sd=1)</code>.</p>
<p>Here, because we are using simulations, we can obtain the standard error of the mean by generating many data sets. Recall that the standard error of the mean is equal to a scaled version of the standard deviation:</p>
<p><span class="math display">\[SE(\hat{\theta}_n) = \frac{\sqrt{\sigma^2}}{\sqrt{n}}\]</span></p>
<p>In practice, when working with a single data set, the estimator’s standard error <span class="math inline">\(SE(\hat{\theta}_n)\)</span> is generally unknown. However, it can be approximated using the sample standard deviation <span class="math inline">\(s\)</span> of the dataset we are studying:</p>
<p><span class="math display">\[SE(\hat{\theta}_n) \approx \frac{s}{\sqrt{n}}\]</span>
Thus, we can obtain CIs by replacing <span class="math inline">\(SE(\hat{\theta}_n)\)</span> above with <span class="math inline">\(s/\sqrt{n}\)</span>:</p>
<p><span class="math display">\[(\hat{\theta} + \frac{s}{\sqrt{n}}q_{2.5\%}, \hat{\theta} + \frac{s}{\sqrt{n}}q_{97.5\%})\]</span></p>
<p><strong>Exercise</strong>: Repeat the exercise above, but instead of using the standard error, approximate this value by using the standard deviation (<span class="math inline">\(s\)</span>) of each data set (via the function <code>sd()</code>). Plug that standard deviation into the formula above for obtaining confidence intervals. How often do these confidence intervals contain the expected value? Repeat this exercise multiple times. Is the proportion of bounded means as accurate as in the previous exercise?</p>
<p>As you’ve probably noticed, this will lead to overly confident boundaries, because we’ve replaced the standard error with a rough approximation to it. To correct for this, we must replace the quantiles of the Normal distribution with the quantiles of a distribution with slightly bigger tails: a ‘more uncertain’ distribution, called the t-distribution. This serves to correct for the extra uncertainty that we are bringing in by using our sample’s standard deviation <span class="math inline">\(s\)</span> instead of the true <span class="math inline">\(SE(\hat{\theta}_n)\)</span> of the sampling distribution:</p>
<p><img src="DataScience_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<p>The t-distribution has a single parameter (called the ‘degrees of freedom’), and represents how much information we have about the shape of the sampling distribution. In our case, this parameter should be set to <span class="math inline">\(n-1\)</span> where <span class="math inline">\(n\)</span> is the size of our data set. The larger our dataset, the bigger this parameter, and the closer the t-distribution will become to a Normal distribution.</p>
<p>Thus, to compute the <span class="math inline">\(95\%\)</span> confidence intervals when working with a real data set with a normally distributed estimator, we can use this formula:</p>
<p><span class="math display">\[(\hat{\theta} + \frac{s}{\sqrt{n}}t_{2.5\%}, \hat{\theta} + \frac{s}{\sqrt{n}}t_{97.5\%})\]</span>
where <span class="math inline">\(t_{x\%}\)</span> is the <span class="math inline">\(x\%\)</span> quantile function of the t-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</p>
<p><strong>Exercise</strong>: Repeat the exercise above, but instead of using the standard normal(0,1) quantiles, use the quantiles from a t-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom, where <span class="math inline">\(n\)</span> is the size of each data set. Hint: You can obtain the quantile of a t-distribution using the function <code>qt()</code>. For example, the <span class="math inline">\(30\%\)</span> quantile of a t-distribution with 4 degrees of freedom is equal to <code>qt(0.3,4)</code>.</p>
<p>This is a useful formula, as the sampling distribution of the sample mean of large data sets (large <span class="math inline">\(n\)</span>) will tend to be normal and unbiased due to the Central Limit Theorem. Thus, we can readily use this formula whenever we’ve obtained a large dataset and have calculated its sample mean and standard deviation!</p>
<p>A good rule of thumb is to use the above formula to compute CIs using the sample mean <span class="math inline">\(\bar{X}\)</span> as the estimator <span class="math inline">\(\hat{\theta}\)</span> if:</p>
<ol style="list-style-type: decimal">
<li><p>The data points <span class="math inline">\(X_1, X_2, ..., X_n\)</span> of your data set are known to come from a Normal distribution, so <span class="math inline">\(\bar{X}\)</span> is guaranteed to be Normally distributed, OR…</p></li>
<li><p>The data points <span class="math inline">\(X_1, X_2, ..., X_n\)</span> of your data set might not come from a Normal distribution, but <span class="math inline">\(n\)</span> is large enough that the Central Limit Theorem kicks in, and so <span class="math inline">\(\bar{X}\)</span> is approximately Normal (generally around 30 data points).</p></li>
</ol>
<p>If these assumptions don’t hold, you might need to use other methods, like permutation and resampling, which we’ll cover in later lectures.</p>
</div>
<div id="hypothesis-testing" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Hypothesis testing</h2>
<p>We often need to use the sampling distribution of an estimator <span class="math inline">\(\hat{\theta}\)</span> to determine whether the value of the estimator is “far enough” from a given value <span class="math inline">\(\theta_0\)</span> to reject the hypothesis that the true expected value is equal to <span class="math inline">\(\theta_0\)</span>. How “far enough” the estimator needs to be from <span class="math inline">\(\theta_0\)</span> - the null hypothesis - will depend on how wide the sampling distribution is.</p>
<p>If we’re dealing with unbiased, normally distributed estimators - like the sample mean when <span class="math inline">\(n\)</span> is large - then under the hypothesis that the true expected value <span class="math inline">\(\theta = \theta_0\)</span>:</p>
<p><span class="math display">\[\hat{\theta} \sim Normal(\theta_0, Var(\hat{\theta}))\]</span>
In other words,</p>
<p><span class="math display">\[\hat{\theta} - \theta_0 \sim Normal(0, Var(\hat{\theta}))\]</span>
If we move the variance to the other side, we obtain:</p>
<p><span class="math display">\[\frac{\hat{\theta} - \theta_0}{\sqrt{Var(\hat{\theta})}} \sim Normal(0, 1)\]</span>
Recalling that the standard error (SE) is equal to the square root of the variance of the estimator, we can also re-write the above formula as:</p>
<p><span class="math display">\[\frac{\hat{\theta} - \theta_0}{SE(\hat{\theta})} \sim Normal(0, 1)\]</span></p>
<p>For example, in the plot below, we show a <span class="math inline">\(Normal(0,1)\)</span> distribution, and an observed value of the difference <span class="math inline">\(\hat{\theta}-\theta_0\)</span>. If the magnitude of this difference is far enough from zero, then we can reject the hypothesis that <span class="math inline">\(\hat{\theta} = \theta_0\)</span>. We need to define an arbitrary cutoff for what “far enough” means. This cutoff is traditionally set such that values as extreme or more extreme than the one observed have a probability less than <span class="math inline">\(5\%\)</span> (in other words, P-value &lt; 0.05), but this cutoff is just a tradition.</p>
<p><img src="DataScience_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<p>We also need to be aware that we often don’t know whether the true expected value of our distribution is <em>a priori</em> lower or higher than the value stated by our null hypothesis. In that case, we need to look at the two tails of the sampling distribution, e.g. the probability mass to the left of the green line and to the right of the red line in the plot below:</p>
<p><img src="DataScience_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
<p>Annoyingly, to determine the “width” of the sampling distribution of the estimator, we need its standard error. If our test statistic is the sample mean and we have a large dataset (generally <span class="math inline">\(n&gt;30\)</span>), then can again re-use the Central Limit Theorem, which states the standard error of the sample mean is equal to the standard deviation scaled by the square root of the sample size:</p>
<p><span class="math display">\[SE(\hat{\theta}) = \sqrt{\sigma^2}/\sqrt{n}\]</span>
Thus:</p>
<p><span class="math display">\[\frac{\hat{\theta} - \theta_0}{\sqrt{\sigma^2/n}} \sim Normal(0, 1)\]</span></p>
<p>This standardized version of the sample mean is called a <strong>z-score</strong> and follows a standard normal distribution. However, the standard deviation in the denominator (<span class="math inline">\(\sqrt{\sigma^2}\)</span>) is generally unavailable because we would only be able to know this with certainty if we knew the parameters of the distribution from which our data comes from. Instead, just as we did when computing confidence intervals, <span class="math inline">\(\sqrt{\sigma^2}\)</span> can be replaced by the standard deviation <strong>of the data set at hand</strong> (s), scaled by the square root of the sample size:</p>
<p><span class="math display">\[SE(\hat{\theta}) \approx s/\sqrt{n}\]</span></p>
<p>When this happens, we can no longer say that our test statistic follows a Normal distribution: the use of the sample standard deviation as a “stand-in” for the true standard deviation causes the sampling distribution of our test statistic to become wider. Our new test-statistic - called a <strong>t-statistic</strong> - now follows a new distribution called the <strong>t-distribution</strong> with <span class="math inline">\(n-1\)</span> degrees of freedom:</p>
<p><span class="math display">\[\frac{\hat{\theta} - \theta_0}{s/\sqrt{n}} \sim t_{n-1}\]</span></p>
<p>The t-distribution is a handy distribution when performing hypothesis testing, particularly because it arises whenever we use the standard deviation of the data as a replacement for the standard error.</p>
<blockquote>
<p>The t-statistic is an example of a ‘pivot statistic’: like other statistics, it can be a function of observations, but its key feature is that its probability distribution does not depend on unknown parameters. You can be sure that the distribution of a t-statistic will always be the t-distribution with n-1 degrees of freedom, even if we don’t know the true standard deviation of the sampling distribution of <span class="math inline">\(\hat{\theta}\)</span>.</p>
</blockquote>
<p>As before, a good rule of thumb is to use the above formula for performing hypothesis testing using the sample mean <span class="math inline">\(\bar{X}\)</span> as the estimator <span class="math inline">\(\hat{\theta}\)</span> if:</p>
<ol style="list-style-type: decimal">
<li><p>The data points <span class="math inline">\(X_1, X_2, ..., X_n\)</span> of your data set are known to come from a Normal distribution, so <span class="math inline">\(\bar{X}\)</span> is guaranteed to be Normally distributed, OR…</p></li>
<li><p>The data points <span class="math inline">\(X_1, X_2, ..., X_n\)</span> of your data set might not come from a Normal distribution, but <span class="math inline">\(n\)</span> is large enough that the Central Limit Theorem kicks in, and so <span class="math inline">\(\bar{X}\)</span> is approximately Normal (generally around 30 data points).</p></li>
</ol>
<p><strong>Exercise</strong>: Create a single simulated data set of size 50, where each data point is drawn from a Normal distribution with expected value equal to 4 and standard deviation equal to 10. Let’s pretend like you don’t know the expected value from which you’re simulating. Using a P-value cutoff of <span class="math inline">\(5\%\)</span>, can you reject the null hypothesis that the expected value is equal to 5? The test will need to be <code>two.sided</code> as we don’t know whether we expect the true expected value to be larger than or smaller than 5 <em>a priori</em>.
Hint: you can do this in two ways, which should yield exactly the same P-values:</p>
<ol style="list-style-type: decimal">
<li><p>computing a t-statistic and then using twice the CDF of a t-distribution with ‘n-1’ degrees of freedom, to obtain the symmetric 2-tailed P-values: <code>2*pt()</code>, or…</p></li>
<li><p>using the handy function <code>t.test()</code>. You’ll need to set the option <code>x</code> to be equal to your vector of data points, and the option <code>mu</code> to the particular hypothesis value you might want to test.</p></li>
</ol>
<p><strong>Exercise</strong>: Repeat the above exercise, but this time simulate a data set of size 500 instead. Using a P-value cutoff of <span class="math inline">\(5\%\)</span>, can you now reject the null hypothesis that the expected value is equal to 5?</p>
<p>This exercise illustrates that the ability to reject a null hypothesis does not only depend on the difference between a statistic and its null expectation, but also in the amount of data we have, i.e. the power we have to reject the hypothesis.</p>
<blockquote>
<p>You might notice lots of similarities between the computation of confidence intervals and the testing of a hypothesis: same assumptions, same approximations, same use of the Normal distribution. Under the “frequentist” school of statistical thought, hypothesis testing and confidence interval estimation are indeed intimately related: they are two sides of the same coin!</p>
</blockquote>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="likelihood-based-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DataScience.pdf", "DataScience.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
