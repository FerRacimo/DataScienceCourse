[["properties-of-estimators-and-inference.html", "Chapter 7 Properties of Estimators and Inference 7.1 Properties of point estimators 7.2 Obtaining confidence intervals 7.3 Hypothesis testing", " Chapter 7 Properties of Estimators and Inference 7.1 Properties of point estimators In this exercise, we’ll draw many simulated samples from a known distribution with known parameters. We will then consider these as instances of real datasets, and estimate parameters of the original distributions using different estimators applied to the datasets. Our goal will be to analyze properties of the estimators, namely their bias and variance. Recall the definitions of these two properties: Bias: \\[B(\\hat\\theta_n) = E[\\hat\\theta_n(D) - \\theta] \\] The bias reflects the average difference between our estimator and the true parameter. Variance: \\[Var(\\hat\\theta_n) = E[ (\\hat\\theta_n(D) - E[\\hat\\theta_n(D)] )^2] \\] The variance reflects the variation of the estimator around its own mean (regardless of what the true parameter is). It is also the square of the standard error. Note: in real life, we will generally not know the distribution from which our data comes from, or the value of the parameters of that distribution (otherwise we wouldn’t be performing statistical inference). When developing or applying a particular estimator to our data, it is thus important to test the properties of that estimator under a variety of simulations and parameter choices. These simulations should aim to, as much as possible, mimic the possible range of processes and parameters that could be generating our data. We will first create a function that draws a user-specified number (nsim) of independent data samples of size sampsize from a distribution of choice. By default this distribution is the Normal distribution (rnorm), but it can be changed by the user: simsamps &lt;- function(sampsize, nsim = 10000, rsim = rnorm, ...){ sims &lt;- rsim(sampsize*nsim,...) # generate draws from a specified distribution simmat &lt;- matrix(sample(sims), nrow=nsim, ncol=sampsize) # organize those draws into a matrix return(simmat) # provide the matrix as output } The output of this function is a matrix, with row number equal to the number of generated simulations, and column number equal to the size of each simulation. For example, to create 10,000 simulations, each of size 30, from a normal distribution, we would write: testmat &lt;- simsamps(30,10000,rnorm) # generate matrix of 10,000 Normal data sets, each of size 30 dim(testmat) # dimensions of matrix ## [1] 10000 30 What type of Normal distribution are we sampling from though? By default, the function rnorm samples from a standard Normal distribution with mean equal to 0 and standard deviation equal to 1. In our function simsamps, we are only feeding one argument to the normal distribution (the number of draws we want to obtain from it). The three dots placed in both the argument of the simsamps function and in its internal call to rsim allows us to feed more parameters to rsim: testmat &lt;- simsamps(30,100,rnorm,mean=3,sd=5) # generate matrix of 1000 Normal(3,5) data sets of size 30 We can obtain the sample mean and sample median from each of these datasets, using the apply function: testmean &lt;- apply(testmat,1,mean) testmedian &lt;- apply(testmat,1,median) Exercise: Create 20,000 simulated data sets of size 5, each drawn from a Normal distribution with expected value equal to 4 and standard deviation equal to 10. Create a histogram of the sample means from all the data sets. Draw a blue line where the average of all these means is located, and a red line where the expected value of the original source distribution is located. Next, do the same for all the sample medians. Now, try answering these questions: Is the sample mean a biased estimator of the expected value of a normal distribution? Is the sample median a biased estimator of the expected value of a normal distribution? Which estimator has the highest variance? Which estimator would you use if given a dataset like one of the ones we simulated, in order to estimate the expected value of this Normal distribution? Exercise: Create 20,000 simulated data sets of size 10, each drawn from an Exponential distribution with rate equal to 2. Create a histogram of the sample means from all the data sets. Draw a blue line where the average of all these means is located, and a red line where the expected value of the original source distribution is located. Recall that the expected value of this distribution is 1/rate = 0.5. Now, try answering these questions: Is the sample mean a biased estimator of the expected value of an exponential distribution? Is the sample median a biased estimator of the expected value of an exponential distribution? Which of these two estimators would you use if you were trying to estimate the rate of an exponential process (e.g. the rate at which buses arrive at a station), from a dataset like one of the ones we just simulated? The histograms we’ve built are examples of sampling distributions of estimators. They serve to visualize the spread of an estimator’s distribution around its own mean, and allow us to determine whether that estimator’s mean is equal to the expected value of our distribution of interest. 7.2 Obtaining confidence intervals Confidence intervals (CIs) denote how sure we are about the value of a parameter. Importantly, CIs are statements made about an infinite number of data sets, which can be a bit counter-intuitive. For example, let’s imagine a parameter of interest called \\(\\theta\\), which has a value that we don’t know. If we had an infinite number of data sets (\\(i=1,2,3,...,\\infty\\)), the lower and upper 95% confidence intervals for each dataset \\(i\\) are the two values that should bound (contain) the unknown parameter \\(\\theta\\) in 95% of those datasets. Each data set will have its own confidence interval, but we can be sure that that interval will contain the unknown parameter 95% of the time. Generally, we only have one set of data points, so this statement might sound a bit confusing. How can we make statements about infinite data sets, when we only have one set? In practice, the confidence interval is an approximation based on the spread (variance) of an estimator’s (\\(\\hat{\\theta}\\)) sampling distribution around the expected value of the unknown parameter (\\(\\theta\\)). If a given estimator \\(\\hat{\\theta}\\) of a data set of size n: has a known standard error: \\[\\omega = SE(\\hat{\\theta}_n) = \\sqrt{Var(\\hat{\\theta}_n)}\\] is unbiased: \\[E[\\hat{\\theta}_n - \\theta] = 0\\] and has a normal sampling distribution: \\[\\hat{\\theta}_n \\sim Normal\\] then the confidence intervals that will contain the true value of the parameter 95% of the time are: \\[(\\hat{\\theta} + \\omega\\ q_{2.5\\%}, \\hat{\\theta} + \\omega\\ q_{97.5\\%})\\] Here, \\(q_{x\\%}\\) is the \\(x\\%\\) quantile function of a standard Normal(0,1) distribution. This value marks a limit such that \\(x\\%\\) of the probability mass of a distribution is to the left (lower than the value), and \\(1-x\\%\\) is to the right (higher than the value). Because the standard normal(0,1) distribution is symmetric and centered around 0, \\(q_{2.5\\%} = -q_{97.5\\%}\\), so we can also write the CI as: \\[(\\hat{\\theta} - \\omega\\ q_{97.5\\%}, \\hat{\\theta} + \\omega\\ q_{97.5\\%})\\] If we compute these boundaries for a particular data set we study, and the estimator we’re applying satisfies the 2 conditions above, we can be sure that these boundaries will contain the true parameter \\(95\\%\\) of the time (out of an infinite number of possible data sets that we could have obtained, in theory, from the phenomenon of interest). Note that some of the sampling distributions we saw in the previous section are neither normal nor unbiased. For example, the sampling distribution of the mean of a data set that is exponentially distributed is not normal, and the distribution of the median of such data set is neither normal nor unbiased! Thus, it would be inappropriate to compute the 95% CIs using the equation above in those cases. Later on in this class, we’ll figure out ways to obtain more general CIs that do not depend on the strict assumptions of unbiasedness and normality of the estimator’s sampling distribution. The mean of a normally distributed dataset is both unbiased and normally distributed, so those assumptions hold in that case. Let’s verify that: Exercise: Create 20,000 simulated data sets of size 5, each drawn from a Normal distribution with expected value equal to 4 and standard deviation equal to 10. For each dataset, compute the sample mean. Then, compute the standard error of the mean (standard deviation over all means). Finally, use the standard error to compute the \\(95\\%\\) confidence intervals for each data set, using the formula stated above. How often do these confidence intervals contain the expected value? Hint: to obtain the quantiles of a standard Normal(x) distribution, you can use the function qnorm. For example, the \\(35\\%\\) quantile of a standard Normal(0,1) distribution is equal to qnorm(0.35,mean=0,sd=1). Because we are using simulations, we can obtain the standard error of the mean by generating many data sets. In practice, when working with a single data set, the standard error \\(\\omega\\) is generally unknown. However, it can be approximated using the sample standard deviation \\(s\\) of the dataset we are studying (using the function sd()): \\[(\\hat{\\theta} + \\frac{s}{\\sqrt{n}}q_{2.5\\%}, \\hat{\\theta} + \\frac{s}{\\sqrt{n}}q_{97.5\\%})\\] Exercise: Repeat the exercise above, but instead of using the standard deviation of each data set to approximate the standard error, compute the standard error of the means across all datasete, then plug that standard error into the formula above for obtaining confidence intervals. How often do these confidence intervals contain the expected value? Repeat this exercise multiple times. Is the proportion of bounded means as accurate as in the previous exercise? As you’ve probably noticed, this will lead to overly confident boundaries, because we’ve replaced the standard error with a rough approximation to it. To correct for this, we must replace the quantiles of the Normal distribution with the quantiles of a distribution with slightly bigger tails: a ‘more uncertain’ distribution, called the t-distribution. This serves to correct for the extra uncertainty that we are bringing in by using the sample standard deviation \\(s\\): The t-distribution has a single parameter (called the ‘degrees of freedom’), and represents how much information we have about the shape of the sampling distribution. In our case, this parameter should be set to \\(n-1\\) where \\(n\\) is the size of our data set. The larger our dataset, the bigger this parameter, and the closer the t-distribution will become to a Normal distribution. Thus, to compute the \\(95\\%\\) confidence intervals when working with a real data set that we know is normally distributed, we can use this formula: \\[(\\hat{\\theta} + \\frac{s}{\\sqrt{n}}t_{2.5\\%}, \\hat{\\theta} + \\frac{s}{\\sqrt{n}}t_{97.5\\%})\\] where \\(t_{x\\%}\\) is the \\(x\\%\\) quantile function of the t-distribution with \\(n-1\\) degrees of freedom. Exercise: Repeat the exercise above, but instead of using the standard normal(0,1) quantiles, use the quantiles from a t-distribution with \\(n-1\\) degrees of freedom, where \\(n\\) is the size of each data set. Hint: You can obtain the quantile of a t-distribution using the function qt(). For example, the \\(30\\%\\) quantile of a t-distribution with 4 degrees of freedom is equal to qt(0.3,4). 7.3 Hypothesis testing "]]
