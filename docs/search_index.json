[["ordination.html", "Chapter 14 Ordination 14.1 Libraries and Data 14.2 Principal component analysis (PCA) 14.3 PCA under the hood 14.4 Principal components as explanatory variables", " Chapter 14 Ordination 14.1 Libraries and Data Today, we will work with the package vegan (useful for ordination techniques) and the packages ggplot2 and ggbiplot (useful for fancy plotting). Make sure all these libraries are installed before you begin. Let’s begin by installing and loading the necessary libraries: if (!require(&quot;vegan&quot;)) install.packages(&quot;vegan&quot;) if (!require(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) if (!require(&quot;ggplot2&quot;)) install.packages(&quot;ggplot2&quot;) We will use a dataset on measurements of particular parts of the iris plant, across individuals from three different species. data(iris) Exercise: Take a look at the iris data matrix. How many samples does it have? How many variables? What happens when you run the function plot() on this matrix? Which variables seem to be strongly correlated? (you can use the function cor() to compute the strength of correlations). Speculate as to why some of these variables could be strongly correlated. 14.2 Principal component analysis (PCA) We’ll perform a PCA of the data. The function prcomp() performs the PCA, and we can assign the result of this function to a new variable (let’s call it “fit”). We must first remove the last column to whatever we give as input to prcomp, as the species names are a non-linear (categorical) variable and we don’t have (for now) any natural measures of distance for species. The option scale=T standardizes the variables to the same relative scale, so that some variables do not become dominant just because of their large measurement unit. We use irisnumvar &lt;- iris[-5] # Remove the categorical variable fit&lt;-prcomp(irisnumvar, scale=TRUE) # Perform PCA Exercise: Try using the summary() and plot() functions to obtain a summary of the resulting PCA object How many principal components were created? (note that the number of PCs always equals the number of original variables). How much variance does the first principal component serve to explain in our data? How much variance does the second component explain? How many PCs would we need to be able to explain at least 95% of the variation in our data? The “Rotation” matrix is included inside the fit object we just constructed. You can retrieve it by typing fit[2]$rotation. This matrix contains the “loadings” of each of the original variables on the newly created PCs. Exercise: Take a look at the rotation matrix. The larger the absolute value of a variable in each PC, the more that variable contributes to that PC. For each component, use the function barplot() to plot the loadings (contributions) of each variable into that component. Which variables contribute most to each component? Exercise: Use the function “biplot” to plot the first two PCs of our data. The plotted arrows provide a graphical rendition of the loadings of each of the original variables on the two PCs. Across this reduced dimensional space, we can see that particular variables tend to co-vary quite strongly. Which ones? We can also see a separation into two groups on PC1. Based on the previous exercise (looking at the rotation matrix), which variables do you think would be most different between samples in one group and in the other? 14.3 PCA under the hood Rather than just using a ready-made function to compute a PCA, let’s take a longer route to understand exactly what’s happening under the hood of the prcomp() function. First, let’s standardize each column of our data so that each column has mean 0 and variance 1 irisdat &lt;- iris[-5] irisstandard &lt;- apply(irisdat,2,function(x){(x-mean(x))/sd(x)}) Now, calculate the covariance matrix. Because the data has been standardized, this is equivalent to calculating the correlation matrix of the pre-standardized data. cormat &lt;- cov(irisstandard) Then, extract the eigenvalues and eigenvectors of correlation matrix: myEig &lt;- eigen(cormat) Now, we’ll manually obtain certain values that were automatically computed by the prcomp function when we ran it earlier. We’ll calculate the singular values (square root of eigenvalues) and also obtain the eigenvectors, also called loadings. sdLONG &lt;- sqrt(myEig$values) loadingsLONG &lt;- myEig$vectors rownames(loadingsLONG) &lt;- colnames(irisstandard) Using the loadings, we can plot our original (standardized) data matrix into the new PC-space, by multiplying the data matrix by the matrix of loadings. Plotting the first two rows of the resulting product should reveal the location of our data points in the first two principal components (like we had before): scoresLONG &lt;- irisstandard %*% loadingsLONG iristoplot &lt;- data.frame(scoresLONG,iris$Species) colnames(iristoplot) &lt;- c(&quot;PC1&quot;,&quot;PC2&quot;,&quot;PC3&quot;,&quot;PC4&quot;,&quot;Species&quot;) ggplot(iristoplot, aes(PC1, PC2)) + geom_point(aes(color=Species, shape = Species)) + xlab(&quot;PC1&quot;) + ylab(&quot;PC2&quot;) + ggtitle(&quot;Iris PCA&quot;) You can compare the results from the first section (using the ready-made function prcomp) and this section (taking a longer road), to check that the results are equivalent. The function range() returns a vector containing the minimum and maximum of a given vector. Using this function, we can observe that the minimum and maximum differences in values for the loadings, the scores and the standard deviations of the PCs are all infinitesimally small (effectively zero). range(fit$sdev - sdLONG) ## [1] -6.661338e-16 2.220446e-16 range(fit$rotation - loadingsLONG) ## [1] -6.661338e-16 7.771561e-16 range(fit$x - scoresLONG) ## [1] -2.359224e-15 3.108624e-15 14.4 Principal components as explanatory variables We can use principal components as explanatory variables to any linear model. In this case, we’ll use the first two principal components of the PCA we performed above, to perform a logistic regression on the probability that an individual belongs to the species ‘virginica.’ First, let’s create a new variable that is equal to 1 if an individual belongs to this species, and is 0 otherwise. We’ll use this variable as the response variable isvirginica &lt;- as.numeric(iris[,5] == &quot;virginica&quot;) We now collate the principal components from the exercise above into a new dataframe that also includes the response variable we just created. # The PC scores are stored in the fifth element of fit. Here, we could have also used the object scoresLONG which we obtained by fitting a PCA manually. PC.scores &lt;- fit[5] newiris &lt;- data.frame(PC.scores,isvirginica) colnames(newiris) &lt;- c(&quot;PC1&quot;,&quot;PC2&quot;,&quot;PC3&quot;,&quot;PC4&quot;,&quot;isvirginica&quot;) head(newiris) ## PC1 PC2 PC3 PC4 isvirginica ## 1 -2.257141 -0.4784238 0.12727962 0.024087508 0 ## 2 -2.074013 0.6718827 0.23382552 0.102662845 0 ## 3 -2.356335 0.3407664 -0.04405390 0.028282305 0 ## 4 -2.291707 0.5953999 -0.09098530 -0.065735340 0 ## 5 -2.381863 -0.6446757 -0.01568565 -0.035802870 0 ## 6 -2.068701 -1.4842053 -0.02687825 0.006586116 0 Exercise: use the glm() function on the newly created newiris data-frame, to perform a logistic regression for the probability that an individual belongs to the species virginica, using the first two principal components (PC1 and PC2) as explanatory variables. Do both components have fitted effects that are significantly different from 0? Do these results make sense in light of the PCA biplots created in the sections above? Exercise: Compare the logistic model to another logistic model, this time using only PC1 as the explanatory variable. Which model has the highest AIC score? "]]
