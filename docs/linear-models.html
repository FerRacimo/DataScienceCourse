<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Linear Models | Data Analysis and Statistical Thinking: An R Workbook</title>
  <meta name="description" content="This is a guide for the Globe Data Science Course." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Linear Models | Data Analysis and Statistical Thinking: An R Workbook" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a guide for the Globe Data Science Course." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Linear Models | Data Analysis and Statistical Thinking: An R Workbook" />
  
  <meta name="twitter:description" content="This is a guide for the Globe Data Science Course." />
  

<meta name="author" content="Fernando Racimo, Shyam Gopalakrishnan" />


<meta name="date" content="2021-05-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability-catch-up.html"/>
<link rel="next" href="properties-of-estimators-and-hypothesis-testing.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis and Statistical Thinking</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Getting Started with Data Analysis</a></li>
<li class="chapter" data-level="3" data-path="prob1.html"><a href="prob1.html"><i class="fa fa-check"></i><b>3</b> Probability Part 1</a>
<ul>
<li class="chapter" data-level="3.1" data-path="prob1.html"><a href="prob1.html#todays-programme"><i class="fa fa-check"></i><b>3.1</b> Today’s programme</a></li>
<li class="chapter" data-level="3.2" data-path="prob1.html"><a href="prob1.html#the-bernoulli-distribution-tossing-a-coin"><i class="fa fa-check"></i><b>3.2</b> The Bernoulli distribution: tossing a coin</a></li>
<li class="chapter" data-level="3.3" data-path="prob1.html"><a href="prob1.html#adding-up-coin-tosses"><i class="fa fa-check"></i><b>3.3</b> Adding up coin tosses</a></li>
<li class="chapter" data-level="3.4" data-path="prob1.html"><a href="prob1.html#the-expectation"><i class="fa fa-check"></i><b>3.4</b> The expectation</a></li>
<li class="chapter" data-level="3.5" data-path="prob1.html"><a href="prob1.html#our-first-probability-mass-function"><i class="fa fa-check"></i><b>3.5</b> Our first probability mass function</a></li>
<li class="chapter" data-level="3.6" data-path="prob1.html"><a href="prob1.html#the-variance"><i class="fa fa-check"></i><b>3.6</b> The variance</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability-part-2.html"><a href="probability-part-2.html"><i class="fa fa-check"></i><b>4</b> Probability Part 2</a></li>
<li class="chapter" data-level="5" data-path="probability-catch-up.html"><a href="probability-catch-up.html"><i class="fa fa-check"></i><b>5</b> Probability Catch-up</a>
<ul>
<li class="chapter" data-level="5.1" data-path="probability-catch-up.html"><a href="probability-catch-up.html#discrete-distributions"><i class="fa fa-check"></i><b>5.1</b> Discrete distributions</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="probability-catch-up.html"><a href="probability-catch-up.html#poisson-distribution"><i class="fa fa-check"></i><b>5.1.1</b> Poisson distribution</a></li>
<li class="chapter" data-level="5.1.2" data-path="probability-catch-up.html"><a href="probability-catch-up.html#geometric-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Geometric distribution</a></li>
<li class="chapter" data-level="5.1.3" data-path="probability-catch-up.html"><a href="probability-catch-up.html#sampling-from-arbitrary-distributions"><i class="fa fa-check"></i><b>5.1.3</b> Sampling from arbitrary distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="probability-catch-up.html"><a href="probability-catch-up.html#the-normal-distribution-and-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2</b> The Normal distribution and the Central Limit Theorem</a></li>
<li class="chapter" data-level="5.3" data-path="probability-catch-up.html"><a href="probability-catch-up.html#the-exponential-distribution"><i class="fa fa-check"></i><b>5.3</b> The exponential distribution</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>6</b> Linear Models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-models.html"><a href="linear-models.html#fitting-a-simple-linear-regression"><i class="fa fa-check"></i><b>6.1</b> Fitting a simple linear regression</a></li>
<li class="chapter" data-level="6.2" data-path="linear-models.html"><a href="linear-models.html#interpreting-a-simple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Interpreting a simple linear regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-models.html"><a href="linear-models.html#simulating-data-from-a-linear-model"><i class="fa fa-check"></i><b>6.3</b> Simulating data from a linear model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="properties-of-estimators-and-hypothesis-testing.html"><a href="properties-of-estimators-and-hypothesis-testing.html"><i class="fa fa-check"></i><b>7</b> Properties of Estimators and Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="properties-of-estimators-and-hypothesis-testing.html"><a href="properties-of-estimators-and-hypothesis-testing.html#properties-of-point-estimators"><i class="fa fa-check"></i><b>7.1</b> Properties of point estimators</a></li>
<li class="chapter" data-level="7.2" data-path="properties-of-estimators-and-hypothesis-testing.html"><a href="properties-of-estimators-and-hypothesis-testing.html#obtaining-confidence-intervals"><i class="fa fa-check"></i><b>7.2</b> Obtaining confidence intervals</a></li>
<li class="chapter" data-level="7.3" data-path="properties-of-estimators-and-hypothesis-testing.html"><a href="properties-of-estimators-and-hypothesis-testing.html#hypothesis-testing"><i class="fa fa-check"></i><b>7.3</b> Hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="likelihood-based-inference.html"><a href="likelihood-based-inference.html"><i class="fa fa-check"></i><b>8</b> Likelihood-based inference</a>
<ul>
<li class="chapter" data-level="8.1" data-path="likelihood-based-inference.html"><a href="likelihood-based-inference.html#meteorite-data"><i class="fa fa-check"></i><b>8.1</b> Meteorite data</a></li>
<li class="chapter" data-level="8.2" data-path="likelihood-based-inference.html"><a href="likelihood-based-inference.html#simple-linear-regression"><i class="fa fa-check"></i><b>8.2</b> Simple linear regression</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>9</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="9.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#using-bayes-rule-covid-19"><i class="fa fa-check"></i><b>9.1</b> Using Bayes’ rule: Covid-19</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#first-foray-into-bayesian-inference"><i class="fa fa-check"></i><b>9.2</b> First foray into Bayesian inference</a></li>
<li class="chapter" data-level="9.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#conjugate-prior-rejection-sampling-and-mcmc"><i class="fa fa-check"></i><b>9.3</b> Conjugate prior, Rejection sampling and MCMC*</a></li>
<li class="chapter" data-level="9.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayesian-point-estimates-and-credible-intervals"><i class="fa fa-check"></i><b>9.4</b> Bayesian point estimates and credible intervals</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#point-estimates"><i class="fa fa-check"></i><b>9.4.1</b> Point estimates</a></li>
<li class="chapter" data-level="9.4.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals"><i class="fa fa-check"></i><b>9.4.2</b> Credible intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>10</b> Classification</a></li>
<li class="chapter" data-level="11" data-path="model-assessment.html"><a href="model-assessment.html"><i class="fa fa-check"></i><b>11</b> Model Assessment</a></li>
<li class="chapter" data-level="12" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>12</b> Resampling</a>
<ul>
<li class="chapter" data-level="12.1" data-path="resampling.html"><a href="resampling.html#the-bootstrap"><i class="fa fa-check"></i><b>12.1</b> The bootstrap</a></li>
<li class="chapter" data-level="12.2" data-path="resampling.html"><a href="resampling.html#permutation-test"><i class="fa fa-check"></i><b>12.2</b> Permutation test</a></li>
<li class="chapter" data-level="12.3" data-path="resampling.html"><a href="resampling.html#validation"><i class="fa fa-check"></i><b>12.3</b> Validation</a></li>
<li class="chapter" data-level="12.4" data-path="resampling.html"><a href="resampling.html#cross-validation"><i class="fa fa-check"></i><b>12.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>13</b> Mixed Models</a></li>
<li class="chapter" data-level="14" data-path="ordination.html"><a href="ordination.html"><i class="fa fa-check"></i><b>14</b> Ordination</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ordination.html"><a href="ordination.html#libraries-and-data"><i class="fa fa-check"></i><b>14.1</b> Libraries and Data</a></li>
<li class="chapter" data-level="14.2" data-path="ordination.html"><a href="ordination.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>14.2</b> Principal component analysis (PCA)</a></li>
<li class="chapter" data-level="14.3" data-path="ordination.html"><a href="ordination.html#pca-under-the-hood"><i class="fa fa-check"></i><b>14.3</b> PCA under the hood</a></li>
<li class="chapter" data-level="14.4" data-path="ordination.html"><a href="ordination.html#principal-components-as-explanatory-variables"><i class="fa fa-check"></i><b>14.4</b> Principal components as explanatory variables</a></li>
<li class="chapter" data-level="14.5" data-path="ordination.html"><a href="ordination.html#nmds"><i class="fa fa-check"></i><b>14.5</b> NMDS</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>15</b> Clustering</a>
<ul>
<li class="chapter" data-level="15.1" data-path="clustering.html"><a href="clustering.html#libraries-and-data-1"><i class="fa fa-check"></i><b>15.1</b> Libraries and Data</a></li>
<li class="chapter" data-level="15.2" data-path="clustering.html"><a href="clustering.html#distances"><i class="fa fa-check"></i><b>15.2</b> Distances</a></li>
<li class="chapter" data-level="15.3" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>15.3</b> K-means clustering</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html"><i class="fa fa-check"></i><b>16</b> REcoStats: Linear Models</a>
<ul>
<li class="chapter" data-level="16.1" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html#fitting-a-simple-linear-regression-1"><i class="fa fa-check"></i><b>16.1</b> Fitting a simple linear regression</a></li>
<li class="chapter" data-level="16.2" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html#interpreting-a-simple-linear-regression-1"><i class="fa fa-check"></i><b>16.2</b> Interpreting a simple linear regression</a></li>
<li class="chapter" data-level="16.3" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html#simulating-data-from-a-linear-model-1"><i class="fa fa-check"></i><b>16.3</b> Simulating data from a linear model</a></li>
<li class="chapter" data-level="16.4" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html#hypothesis-testing-and-permutation-testing"><i class="fa fa-check"></i><b>16.4</b> Hypothesis testing and permutation testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis and Statistical Thinking: An R Workbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-models" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Linear Models</h1>
<p>We describe linear models in this chapter. First we need to load some libraries (and install them if necessary).</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="linear-models.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(<span class="st">&quot;tidyverse&quot;</span>)) <span class="fu">install.packages</span>(<span class="st">&quot;tidyverse&quot;</span>) <span class="co"># Library for data analysis</span></span>
<span id="cb44-2"><a href="linear-models.html#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(<span class="st">&quot;stargazer&quot;</span>)) <span class="fu">install.packages</span>(<span class="st">&quot;stargazer&quot;</span>) <span class="co"># Library for producing pretty tables of estimates from linear models</span></span>
<span id="cb44-3"><a href="linear-models.html#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(<span class="st">&quot;devtools&quot;</span>)) <span class="fu">install.packages</span>(<span class="st">&quot;devtools&quot;</span>)</span>
<span id="cb44-4"><a href="linear-models.html#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(<span class="st">&quot;report&quot;</span>)) devtools<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&quot;easystats/report&quot;</span>) <span class="co"># Library for producing nice verbose reports of linear models</span></span></code></pre></div>
<div id="fitting-a-simple-linear-regression" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Fitting a simple linear regression</h2>
<p>We’ll use a dataset published by Allison and Cicchetti (1976). In this study, the authors studied the relationship between sleep and various ecological and morphological variables across a set of mammalian species:
<a href="https://science.sciencemag.org/content/194/4266/732" class="uri">https://science.sciencemag.org/content/194/4266/732</a></p>
<p>Let’s start by loading the data into a table:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="linear-models.html#cb45-1" aria-hidden="true" tabindex="-1"></a>allisontab <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Data_allison.csv&quot;</span>)</span></code></pre></div>
<p>This dataset contains several variables related to various body measurements and measures of sleep in different species. Note that some of these are continuous, while others are discrete and ordinal.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="linear-models.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(allisontab)</span></code></pre></div>
<pre><code>##    Species              BodyWt            BrainWt         NonDreaming    
##  Length:62          Min.   :   0.005   Min.   :   0.14   Min.   : 2.100  
##  Class :character   1st Qu.:   0.600   1st Qu.:   4.25   1st Qu.: 6.250  
##  Mode  :character   Median :   3.342   Median :  17.25   Median : 8.350  
##                     Mean   : 198.790   Mean   : 283.13   Mean   : 8.673  
##                     3rd Qu.:  48.202   3rd Qu.: 166.00   3rd Qu.:11.000  
##                     Max.   :6654.000   Max.   :5712.00   Max.   :17.900  
##                                                          NA&#39;s   :14      
##     Dreaming       TotalSleep       LifeSpan         Gestation     
##  Min.   :0.000   Min.   : 2.60   Min.   :  2.000   Min.   : 12.00  
##  1st Qu.:0.900   1st Qu.: 8.05   1st Qu.:  6.625   1st Qu.: 35.75  
##  Median :1.800   Median :10.45   Median : 15.100   Median : 79.00  
##  Mean   :1.972   Mean   :10.53   Mean   : 19.878   Mean   :142.35  
##  3rd Qu.:2.550   3rd Qu.:13.20   3rd Qu.: 27.750   3rd Qu.:207.50  
##  Max.   :6.600   Max.   :19.90   Max.   :100.000   Max.   :645.00  
##  NA&#39;s   :12      NA&#39;s   :4       NA&#39;s   :4         NA&#39;s   :4       
##    Predation        Exposure         Danger     
##  Min.   :1.000   Min.   :1.000   Min.   :1.000  
##  1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.000  
##  Median :3.000   Median :2.000   Median :2.000  
##  Mean   :2.871   Mean   :2.419   Mean   :2.613  
##  3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000  
##  Max.   :5.000   Max.   :5.000   Max.   :5.000  
## </code></pre>
<p>We’ll begin by focusing on the relationship between two of the continuous variables: body size (in kg) and total amount of sleep (in hours). Let’s plot these to see what they look like:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="linear-models.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(allisontab) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>BodyWt,<span class="at">y=</span>TotalSleep))</span></code></pre></div>
<pre><code>## Warning: Removed 4 rows containing missing values (geom_point).</code></pre>
<p><img src="DataScience_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>Hmmm this looks weird. We have many measurements of body weight around 0 (small values) and a few very large values of thousands of kilograms. This is not surprising: given that this dataset spans several different species, the measurements spans several orders of magnitude (from elephants to molerats). To account for this, variables involving body measurements (like weight or length) are traditionally converted into a log-scale when fitted into a linear model. Let’s see what happens when we log-scale the body weight variable:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="linear-models.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(allisontab) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span><span class="fu">log</span>(BodyWt),<span class="at">y=</span>TotalSleep))</span></code></pre></div>
<pre><code>## Warning: Removed 4 rows containing missing values (geom_point).</code></pre>
<p><img src="DataScience_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>A pattern appears to emerge now. There seems to be a negative correlation between the log of body weight and the amount of sleep a species has. Indeed, we can measure this correlation using the <code>cor()</code> function:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="linear-models.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(<span class="fu">log</span>(allisontab<span class="sc">$</span>BodyWt), allisontab<span class="sc">$</span>TotalSleep, <span class="at">use=</span><span class="st">&quot;complete.obs&quot;</span>)</span></code></pre></div>
<pre><code>## [1] -0.5328345</code></pre>
<p>Let’s build a simple linear model to explain total sleep, as a function of body weight. In R, the standard way to fit a linear model is using the function <code>lm()</code>. We do so by following the following formula:</p>
<p><code>fit &lt;- lm(formula, data)</code></p>
<p>The formula within an <code>lm()</code> function for a simple linear regression is:</p>
<p><span class="math display">\[\bf{y} \sim \bf{x_1}\]</span>
Where <span class="math inline">\(y\)</span> is the response variable and <span class="math inline">\(x_1\)</span> is the predictor variable. This formula is a shorthand way that R uses for writing the linear regression formula:</p>
<p><span class="math display">\[\bf{Y} = \beta_0 + \beta_1 \bf{x_1} + \bf{\epsilon}\]</span>
In other words, R implicitly knows that each predictor variable will have an associated <span class="math inline">\(\beta\)</span> coefficient that we’re trying to estimate. Note that here <span class="math inline">\(\bf{y}\)</span>, <span class="math inline">\(\bf{x_1}\)</span>, <span class="math inline">\(\bf{\epsilon}\)</span>, etc. represent lists (vectors) of variables. We don’t need to specify additional terms for the <span class="math inline">\(\beta_0\)</span> (intercept) and <span class="math inline">\(\bf{\epsilon}\)</span> (error) terms. The <code>lm()</code> function automatically accounts for the fact that a regression should have an intercept, and that there will necessarily exist errors (residuals) between our fit and the the observed value of <span class="math inline">\(\bf{Y}\)</span>.</p>
<p>We can also write this exact same equation by focusing on a single (example) variable, say <span class="math inline">\(y_i\)</span>:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_{1,i} + \epsilon_i\]</span>
In general, when we talk about vectors of variables, we’ll use boldface, unlike when referring to a single variable.</p>
<p>In our case, we’ll attempt to fit total sleep as a function of the log of body weight, plus some noise:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="linear-models.html#cb54-1" aria-hidden="true" tabindex="-1"></a>myfirstmodel <span class="ot">&lt;-</span> <span class="fu">lm</span>(TotalSleep <span class="sc">~</span> <span class="fu">log</span>(BodyWt), <span class="at">data=</span>allisontab) </span>
<span id="cb54-2"><a href="linear-models.html#cb54-2" aria-hidden="true" tabindex="-1"></a>myfirstmodel</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = TotalSleep ~ log(BodyWt), data = allisontab)
## 
## Coefficients:
## (Intercept)  log(BodyWt)  
##     11.4377      -0.7931</code></pre>
<p>This way, we are fitting the following model:</p>
<p><span class="math display">\[\bf{TotalSleep} = \beta_0 + \beta_1 \bf{log(BodyWt)} + \bf{\epsilon}\]</span>
Remember that the <span class="math inline">\(\beta_0\)</span> coefficient is implicitly assumed by the <code>lm()</code> function. We can be more explicit and incorporate it into our equation, by simply adding a value of 1 (a constant). This will result in exactly the same output as before:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="linear-models.html#cb56-1" aria-hidden="true" tabindex="-1"></a>myfirstmodel <span class="ot">&lt;-</span> <span class="fu">lm</span>(TotalSleep <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> <span class="fu">log</span>(BodyWt), <span class="at">data=</span>allisontab)  </span>
<span id="cb56-2"><a href="linear-models.html#cb56-2" aria-hidden="true" tabindex="-1"></a>myfirstmodel</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = TotalSleep ~ 1 + log(BodyWt), data = allisontab)
## 
## Coefficients:
## (Intercept)  log(BodyWt)  
##     11.4377      -0.7931</code></pre>
<p><strong>Exercise</strong>: the function <code>attributes()</code> allows us to unpack all the components of the object outputted by the function <code>lm()</code> (and many other objects in R). Try inputting your model output into this function. We can observe that one of the attributes of the object is called <code>coefficients</code>. If we type <code>myfirstmodel$coefficients</code>, we obtain a vector with the value of our two fitted coefficients (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>). Using the values from this vector, try plotting the line of best fit on top of the data. Hint: use the <code>geom_abline()</code> function from the <code>ggplot2</code> library.</p>
</div>
<div id="interpreting-a-simple-linear-regression" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Interpreting a simple linear regression</h2>
<p>We can obtain information about our model’s fit using the function <code>summary()</code>:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="linear-models.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(myfirstmodel)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = TotalSleep ~ 1 + log(BodyWt), data = allisontab)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.6990 -2.6264 -0.2441  2.1700  9.9095 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  11.4377     0.5510  20.759  &lt; 2e-16 ***
## log(BodyWt)  -0.7931     0.1683  -4.712 1.66e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.933 on 56 degrees of freedom
##   (4 observations deleted due to missingness)
## Multiple R-squared:  0.2839, Adjusted R-squared:  0.2711 
## F-statistic:  22.2 on 1 and 56 DF,  p-value: 1.664e-05</code></pre>
<p>The <code>summary()</code> function provides a summary of the output of <code>lm()</code> after it’s been given some data and a model to fit. Let’s pause and analyze the output here. The first line just re-states the formula we have provided to fit our model. Below that, we get a summary (min, max, median, etc.) of all the residuals (error terms) between our linear fit and the observed values of <span class="math inline">\(\bf{TotalSleep}\)</span>.</p>
<p>Below that, we can see a table with point estimates, standard errors, and a few other properties of our estimated coefficients: the intercept (<span class="math inline">\(\beta_0\)</span>, first line) and the slope (<span class="math inline">\(\beta_1\)</span>, second line). The standard error is a measure of how confident we are about our point estimate (we’ll revisit this in later lectures). The “t value” corresponds to the statistic for a “t-test” which serves to determine whether the estimate can be considered as significantly different from zero. The last column is the P-value from this test. We can see that both estimates are quite significantly different from zero (P &lt; 0.001), meaning we can reject the hypothesis that these estimates are equivalent to zero.</p>
<p>Finally, the last few lines are overall measures of the fit of the model. ‘Multiple R-squared’ is the fraction of the variance in <span class="math inline">\(\bf{TotalSleep}\)</span> explained by the fitted model. Generally, we want this number to be high, but it is possible to have very complex models with very high R-squared but lots of parameters, and therefore we run the risk of “over-fitting” our data. ‘Adjusted R-squared’ is a modified version of R-squared that attempts to penalize very complex models. The ‘residual standard error’ is the sum of the squares of the residuals (errors) over all observed data points, scaled by the degrees of freedom of the linear model, which is equal to n – k – 1 where n = total observations and k = total model parameters. Finally, the F-statistic is a test for whether <em>any</em> of the explanatory variables included in the model have a relationship to the outcome. In this case, we only have a single explanatory variable (<span class="math inline">\(\bf{log(BodyWt)}\)</span>), and so the P-value of this test is simply equal to the P-value of the t-test for the slope of <span class="math inline">\(\bf{log(BodyWt)}\)</span>.</p>
<p>We can use the function <code>report()</code> from the library <code>easystats</code> (<a href="https://github.com/easystats/report" class="uri">https://github.com/easystats/report</a>) to get a more verbose report than the <code>summary()</code> function provides.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="linear-models.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">report</span>(myfirstmodel)</span></code></pre></div>
<pre><code>## Formula contains log- or sqrt-terms. See help(&quot;standardize&quot;) for how such terms are standardized.
## Formula contains log- or sqrt-terms. See help(&quot;standardize&quot;) for how such terms are standardized.</code></pre>
<pre><code>## We fitted a linear model (estimated using OLS) to predict TotalSleep with BodyWt (formula: TotalSleep ~ 1 + log(BodyWt)). The model explains a significant and substantial proportion of variance (R2 = 0.28, F(1, 56) = 22.20, p &lt; .001, adj. R2 = 0.27). The model&#39;s intercept, corresponding to BodyWt = 0, is at 11.44 (95% CI [10.33, 12.54], t(56) = 20.76, p &lt; .001). Within this model:
## 
##   - The effect of BodyWt [log] is significantly negative (beta = -0.79, 95% CI [-1.13, -0.46], t(56) = -4.71, p &lt; .001; Std. beta = -1.16, 95% CI [-1.91, -0.41])
## 
## Standardized parameters were obtained by fitting the model on a standardized version of the dataset.</code></pre>
<p>Note that this function “standardizes” the input variables before providing a summary of the output, which makes the estimates’ value to be slightly different than those stored in the output of <code>lm()</code>. This makes interpretation of the coefficients easier, as they are now expressed in terms of standard deviations from the mean.</p>
<p>Another way to summarize our output is via a summary table in , which can be easily constructed using the function <code>stargazer()</code> from the library <code>stargazer</code> (<a href="https://cran.r-project.org/web/packages/stargazer/index.html" class="uri">https://cran.r-project.org/web/packages/stargazer/index.html</a>).</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="linear-models.html#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">stargazer</span>(myfirstmodel, <span class="at">type=</span><span class="st">&quot;text&quot;</span>)</span></code></pre></div>
<pre><code>## 
## ===============================================
##                         Dependent variable:    
##                     ---------------------------
##                             TotalSleep         
## -----------------------------------------------
## log(BodyWt)                  -0.793***         
##                               (0.168)          
##                                                
## Constant                     11.438***         
##                               (0.551)          
##                                                
## -----------------------------------------------
## Observations                    58             
## R2                             0.284           
## Adjusted R2                    0.271           
## Residual Std. Error       3.933 (df = 56)      
## F Statistic           22.203*** (df = 1; 56)   
## ===============================================
## Note:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>This package also supports LaTeX and HTML/CSS format (see the <code>type</code> option in <code>?stargazer</code>), which makes it very handy when copying the output of a regression from R into a working document.</p>
<p><strong>Exercise</strong>: try fitting a linear model for <span class="math inline">\(\bf{TotalSleep}\)</span> as a function of brain weight (<span class="math inline">\(\bf{BrainWt}\)</span>). Keep in mind that this is a size measurement that might span multiple orders of magnitude, just like body weight. What are the estimated slope and intercept coefficients? Which coefficients are significantly different from zero? What is the proportion of explained variance? How does this compare to our previous model including <span class="math inline">\(\bf{BodyWt}\)</span>?</p>
<p><strong>Exercise</strong>: Plot the linear regression line of the above exercise on top of your data.</p>
</div>
<div id="simulating-data-from-a-linear-model" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Simulating data from a linear model</h2>
<p>It is often useful to simulate data from a model to understand how its parameters relate to features of the data, and to see what happens when we change those parameters. We will now create a function that can simulate data from a simple linear model. We will then feed this function different values of the parameters, and see what the data simulated under a given model looks like.</p>
<p>Let’s start by first creating the simulation function. We’ll simulate data from a linear model. The model simulation function needs to be told:
1) The number (<span class="math inline">\(n\)</span>) of data points we will simulate
1) How the explanatory variables are distributed: we’ll use a normal distribution to specify this.
2) What the intercept (<span class="math inline">\(\beta_0\)</span>) and slope (<span class="math inline">\(\beta_1\)</span>) for the linear relationship between the explanatory and response variables are
3) How departures (errors) from linearity for the response variables will be modeled: we’ll use another normal distribution for that as well, and control the amount of error using a variable called <code>sigma.res</code>. We’ll assume errors are homoscedastic (have the same variance) in this exercise.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="linear-models.html#cb65-1" aria-hidden="true" tabindex="-1"></a>linearmodsim <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">n=</span><span class="dv">2</span>, <span class="at">beta_0=</span><span class="dv">0</span>, <span class="at">beta_1=</span><span class="dv">1</span>, <span class="at">sigma.res=</span><span class="dv">1</span>, <span class="at">mu.explan=</span><span class="dv">5</span>, <span class="at">sigma.explan=</span><span class="dv">1</span>, <span class="at">rerror=</span>rnorm, <span class="at">r_explan =</span> rnorm, <span class="at">hetero =</span> <span class="dv">0</span> ){</span>
<span id="cb65-2"><a href="linear-models.html#cb65-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Simulate explanatory variables</span></span>
<span id="cb65-3"><a href="linear-models.html#cb65-3" aria-hidden="true" tabindex="-1"></a>  explan <span class="ot">&lt;-</span> <span class="fu">r_explan</span>(n,mu.explan,sigma.explan)</span>
<span id="cb65-4"><a href="linear-models.html#cb65-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Sort the simulated explanatory values from smallest to largest</span></span>
<span id="cb65-5"><a href="linear-models.html#cb65-5" aria-hidden="true" tabindex="-1"></a>  explan <span class="ot">&lt;-</span> <span class="fu">sort</span>(explan)</span>
<span id="cb65-6"><a href="linear-models.html#cb65-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Standardize the response variables so that they are  mean-centered and scaled by their standard deviation</span></span>
<span id="cb65-7"><a href="linear-models.html#cb65-7" aria-hidden="true" tabindex="-1"></a>  explan.scaled <span class="ot">&lt;-</span> <span class="fu">scale</span>(explan)</span>
<span id="cb65-8"><a href="linear-models.html#cb65-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># OPTIONAL: If errors are heteroscedastic (hetero does not equal 0), then their standard deviation will not be constant, and will depend on the explanatory variables </span></span>
<span id="cb65-9"><a href="linear-models.html#cb65-9" aria-hidden="true" tabindex="-1"></a>  sdev.err <span class="ot">&lt;-</span> <span class="fu">sapply</span>(sigma.res <span class="sc">+</span> explan.scaled<span class="sc">*</span>hetero,max,<span class="dv">0</span>)</span>
<span id="cb65-10"><a href="linear-models.html#cb65-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Simulate the error values using the above-specified standard deviation</span></span>
<span id="cb65-11"><a href="linear-models.html#cb65-11" aria-hidden="true" tabindex="-1"></a>  error <span class="ot">&lt;-</span> <span class="fu">rerror</span>(n,<span class="dv">0</span>,sdev.err)</span>
<span id="cb65-12"><a href="linear-models.html#cb65-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Simulate response variables via the linear model</span></span>
<span id="cb65-13"><a href="linear-models.html#cb65-13" aria-hidden="true" tabindex="-1"></a>  response <span class="ot">&lt;-</span> beta_0 <span class="sc">+</span> beta_1 <span class="sc">*</span> explan <span class="sc">+</span> error</span>
<span id="cb65-14"><a href="linear-models.html#cb65-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Output a table containing the explanatory values and their corresponding response values</span></span>
<span id="cb65-15"><a href="linear-models.html#cb65-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(<span class="fu">data.frame</span>(explan,response))</span>
<span id="cb65-16"><a href="linear-models.html#cb65-16" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><strong>Exercise</strong>:</p>
<ol style="list-style-type: lower-alpha">
<li>Carefully read the code for the function above. Make sure you understand every step in the function.<br />
</li>
<li>Plot the output of a simulated linear model with 40 data points, an intercept of 1.5 and a slope of 3. Simulate from the same model one more time, and plot the output again.<br />
</li>
<li>Now, fit the data from your latest simulation using the <code>lm()</code> function. Does your fit match your simulations?<br />
</li>
<li>Try increasing the sample size (say, to 200 data points), and repeat the <code>lm()</code> fitting. How does this influence the accuracy of your fitted model? Try simulating and fitting multiple times to get an idea of how well you can estimate the parameters.<br />
</li>
<li>Try changing the standard deviation of the simulated residual errors (make <code>sigma.res</code> smaller or larger), and repeat the <code>lm()</code> fitting. How does this influence the accuracy of your fitted model?</li>
</ol>
<!-- 
## Optional: A multivariate linear regression

Now we can start making our model a bit more complex. Let's say we want to incorporate another variable - life span (in years) - into the model predicting sleep:


```r
twopredmodelA <- lm(TotalSleep ~ 1 + log(BodyWt) + LifeSpan, data=allisontab)
summary(twopredmodelA)
```

```
## 
## Call:
## lm(formula = TotalSleep ~ 1 + log(BodyWt) + LifeSpan, data = allisontab)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.9130 -3.1805 -0.1834  2.0354  9.2555 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 11.88156    0.81936  14.501  < 2e-16 ***
## log(BodyWt) -0.69097    0.22420  -3.082  0.00331 ** 
## LifeSpan    -0.02972    0.03766  -0.789  0.43375    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4.015 on 51 degrees of freedom
##   (8 observations deleted due to missingness)
## Multiple R-squared:  0.2989, Adjusted R-squared:  0.2714 
## F-statistic: 10.87 on 2 and 51 DF,  p-value: 0.000117
```

In this model, we are now fitting $\bf{TotalSleep}$ as a linear function of two variables $\bf{log(BodyWt)}$ and $\bf{LifeSpan}$. The fitted coefficient for the $\bf{LifeSpan}$ variable is not significantly different from zero. The proportion of explained variance or the residual error have not changed much, relative to our earlier simpler (1-parameter) model. It seems like this new variable is not very useful for modeling $\bf{TotalSleep}$.

In their paper, Allison et al. found that the log of the gestation time in years was a very good predictor of the amount of sleep in a species. Let's what happens when we add this variable into our model instead:


```r
twopredmodelB <- lm(TotalSleep ~ 1 + log(BodyWt) + log(Gestation), data=allisontab)
summary(twopredmodelB)
```

```
## 
## Call:
## lm(formula = TotalSleep ~ 1 + log(BodyWt) + log(Gestation), data = allisontab)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.6216 -2.6310 -0.0214  1.9363  7.7286 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)     20.4156     2.8242   7.229 2.37e-09 ***
## log(BodyWt)     -0.3592     0.2246  -1.599  0.11591    
## log(Gestation)  -2.1502     0.6745  -3.188  0.00245 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.44 on 51 degrees of freedom
##   (8 observations deleted due to missingness)
## Multiple R-squared:  0.4672, Adjusted R-squared:  0.4463 
## F-statistic: 22.36 on 2 and 51 DF,  p-value: 1.067e-07
```

We can see that in a model including both the log of body weight and the log of gestation time, the body weight's regression coefficient is no longer significantly different from zero, but the gestation time's coefficient is. The R-squared and adjusted R-squared values are now much larger than what we've seen before (around 45%!) and the residula standard error is also smaller than before (3.44). It seems like in a model containing both variables, we no longer need body weight to explain sleep duration, we've found a better explanatory variable!

**Exercise**: what happens if we use all three variables ($\bf{LifeSpan}$, $\bf{log(BodyWt)}$ and $\bf{log(Gestation)}$) as explanatory variables? Do we get a much better model fit than we using only two? What if we use $\bf{log(Gestation)}$ as the only explanatory variable?

Keep in mind that we generally want a good fit with as simple a model as possible (Occam's Razor). There will always generally be a trade-off between how much variance the model can explain and how complex the model is (how many parameters it has).


-->
<!-- 

## Hypothesis testing

## Interactions


```r
interactionmodel <- lm(TotalSleep ~ 1 + log(BodyWt) + log(Gestation) + log(BodyWt)*log(Gestation), data=allisontab) 
summary(interactionmodel)
```

```
## 
## Call:
## lm(formula = TotalSleep ~ 1 + log(BodyWt) + log(Gestation) + 
##     log(BodyWt) * log(Gestation), data = allisontab)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.4123 -2.3591 -0.0327  1.7054  7.3174 
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(>|t|)    
## (Intercept)                 19.6560     2.8919   6.797 1.25e-08 ***
## log(BodyWt)                  0.4218     0.7153   0.590  0.55809    
## log(Gestation)              -1.9008     0.7065  -2.690  0.00968 ** 
## log(BodyWt):log(Gestation)  -0.1670     0.1453  -1.150  0.25581    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.429 on 50 degrees of freedom
##   (8 observations deleted due to missingness)
## Multiple R-squared:  0.4809, Adjusted R-squared:  0.4497 
## F-statistic: 15.44 on 3 and 50 DF,  p-value: 3.085e-07
```

## ANOVA
-->
<!-- 

## Working with tidymodels

Tidymodels version:

library("tidymodels")


```r
if (!require("tidymodels")) install.packages("tidymodels")
```

```
## Loading required package: tidymodels
```

```
## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.1 ──
```

```
## ✓ broom     0.7.2      ✓ recipes   0.1.14
## ✓ dials     0.0.9      ✓ rsample   0.0.8 
## ✓ infer     0.5.3      ✓ tune      0.1.1 
## ✓ modeldata 0.0.2      ✓ workflows 0.2.1 
## ✓ parsnip   0.1.3      ✓ yardstick 0.0.7
```

```
## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
## x recipes::check()  masks devtools::check()
## x scales::discard() masks purrr::discard()
## x dplyr::filter()   masks stats::filter()
## x recipes::fixed()  masks stringr::fixed()
## x dplyr::lag()      masks stats::lag()
## x yardstick::spec() masks readr::spec()
## x recipes::step()   masks stats::step()
```

```r
library("tidymodels")
```



```r
lm_fit <- 
  linear_reg() %>% 
  set_engine("lm") %>%
  fit(TotalSleep ~ log(BodyWt) + LifeSpan, data=allisontab)
lm_fit
```

```
## parsnip model object
## 
## Fit time:  2ms 
## 
## Call:
## stats::lm(formula = TotalSleep ~ log(BodyWt) + LifeSpan, data = data)
## 
## Coefficients:
## (Intercept)  log(BodyWt)     LifeSpan  
##    11.88156     -0.69097     -0.02972
```

-->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability-catch-up.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="properties-of-estimators-and-hypothesis-testing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DataScience.pdf", "DataScience.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
