<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Properties of Estimators and Inference | Data Analysis and Statistical Thinking: An R Workbook</title>
  <meta name="description" content="This is a guide for the Globe Data Science Course." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Properties of Estimators and Inference | Data Analysis and Statistical Thinking: An R Workbook" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a guide for the Globe Data Science Course." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Properties of Estimators and Inference | Data Analysis and Statistical Thinking: An R Workbook" />
  
  <meta name="twitter:description" content="This is a guide for the Globe Data Science Course." />
  

<meta name="author" content="Fernando Racimo, …" />


<meta name="date" content="2021-04-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-models.html"/>
<link rel="next" href="frequentist-inference.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis and Statistical Thinking</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Getting Started with Data Analysis</a></li>
<li class="chapter" data-level="3" data-path="prob1.html"><a href="prob1.html"><i class="fa fa-check"></i><b>3</b> Probability Part 1</a>
<ul>
<li class="chapter" data-level="3.1" data-path="prob1.html"><a href="prob1.html#todays-programme"><i class="fa fa-check"></i><b>3.1</b> Today’s programme</a></li>
<li class="chapter" data-level="3.2" data-path="prob1.html"><a href="prob1.html#the-bernoulli-distribution-tossing-a-coin"><i class="fa fa-check"></i><b>3.2</b> The Bernoulli distribution: tossing a coin</a></li>
<li class="chapter" data-level="3.3" data-path="prob1.html"><a href="prob1.html#adding-up-coin-tosses"><i class="fa fa-check"></i><b>3.3</b> Adding up coin tosses</a></li>
<li class="chapter" data-level="3.4" data-path="prob1.html"><a href="prob1.html#the-expectation"><i class="fa fa-check"></i><b>3.4</b> The expectation</a></li>
<li class="chapter" data-level="3.5" data-path="prob1.html"><a href="prob1.html#our-first-probability-mass-function"><i class="fa fa-check"></i><b>3.5</b> Our first probability mass function</a></li>
<li class="chapter" data-level="3.6" data-path="prob1.html"><a href="prob1.html#the-variance"><i class="fa fa-check"></i><b>3.6</b> The variance</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability-part-2.html"><a href="probability-part-2.html"><i class="fa fa-check"></i><b>4</b> Probability Part 2</a></li>
<li class="chapter" data-level="5" data-path="probability-catch-up.html"><a href="probability-catch-up.html"><i class="fa fa-check"></i><b>5</b> Probability Catch-up</a>
<ul>
<li class="chapter" data-level="5.1" data-path="probability-catch-up.html"><a href="probability-catch-up.html#discrete-distributions"><i class="fa fa-check"></i><b>5.1</b> Discrete distributions</a></li>
<li class="chapter" data-level="5.2" data-path="probability-catch-up.html"><a href="probability-catch-up.html#the-normal-distribuiton-and-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2</b> The Normal distribuiton and the Central Limit Theorem</a></li>
<li class="chapter" data-level="5.3" data-path="probability-catch-up.html"><a href="probability-catch-up.html#the-exponential-distribution"><i class="fa fa-check"></i><b>5.3</b> The exponential distribution</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>6</b> Linear Models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-models.html"><a href="linear-models.html#fitting-a-simple-linear-regression"><i class="fa fa-check"></i><b>6.1</b> Fitting a simple linear regression</a></li>
<li class="chapter" data-level="6.2" data-path="linear-models.html"><a href="linear-models.html#interpreting-a-simple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Interpreting a simple linear regression</a></li>
<li class="chapter" data-level="6.3" data-path="linear-models.html"><a href="linear-models.html#simulating-data-from-a-linear-model"><i class="fa fa-check"></i><b>6.3</b> Simulating data from a linear model</a></li>
<li class="chapter" data-level="6.4" data-path="linear-models.html"><a href="linear-models.html#optional-a-multivariate-linear-regression"><i class="fa fa-check"></i><b>6.4</b> Optional: A multivariate linear regression</a></li>
<li class="chapter" data-level="6.5" data-path="linear-models.html"><a href="linear-models.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.5</b> Hypothesis testing</a></li>
<li class="chapter" data-level="6.6" data-path="linear-models.html"><a href="linear-models.html#interactions"><i class="fa fa-check"></i><b>6.6</b> Interactions</a></li>
<li class="chapter" data-level="6.7" data-path="linear-models.html"><a href="linear-models.html#anova"><i class="fa fa-check"></i><b>6.7</b> ANOVA</a></li>
<li class="chapter" data-level="6.8" data-path="linear-models.html"><a href="linear-models.html#working-with-tidymodels"><i class="fa fa-check"></i><b>6.8</b> Working with tidymodels</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="properties-of-estimators-and-inference.html"><a href="properties-of-estimators-and-inference.html"><i class="fa fa-check"></i><b>7</b> Properties of Estimators and Inference</a>
<ul>
<li class="chapter" data-level="7.1" data-path="properties-of-estimators-and-inference.html"><a href="properties-of-estimators-and-inference.html#properties-of-point-estimators"><i class="fa fa-check"></i><b>7.1</b> Properties of point estimators</a></li>
<li class="chapter" data-level="7.2" data-path="properties-of-estimators-and-inference.html"><a href="properties-of-estimators-and-inference.html#obtaining-confidence-intervals"><i class="fa fa-check"></i><b>7.2</b> Obtaining confidence intervals</a></li>
<li class="chapter" data-level="7.3" data-path="properties-of-estimators-and-inference.html"><a href="properties-of-estimators-and-inference.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>7.3</b> Hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="frequentist-inference.html"><a href="frequentist-inference.html"><i class="fa fa-check"></i><b>8</b> Frequentist inference</a></li>
<li class="chapter" data-level="9" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>9</b> Bayesian Inference</a></li>
<li class="chapter" data-level="10" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>10</b> Classification</a></li>
<li class="chapter" data-level="11" data-path="model-assessment.html"><a href="model-assessment.html"><i class="fa fa-check"></i><b>11</b> Model Assessment</a></li>
<li class="chapter" data-level="12" data-path="resampling.html"><a href="resampling.html"><i class="fa fa-check"></i><b>12</b> Resampling</a>
<ul>
<li class="chapter" data-level="12.1" data-path="resampling.html"><a href="resampling.html#the-bootstrap"><i class="fa fa-check"></i><b>12.1</b> The bootstrap</a></li>
<li class="chapter" data-level="12.2" data-path="resampling.html"><a href="resampling.html#permutation-test"><i class="fa fa-check"></i><b>12.2</b> Permutation test</a></li>
<li class="chapter" data-level="12.3" data-path="resampling.html"><a href="resampling.html#validation"><i class="fa fa-check"></i><b>12.3</b> Validation</a></li>
<li class="chapter" data-level="12.4" data-path="resampling.html"><a href="resampling.html#cross-validation"><i class="fa fa-check"></i><b>12.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>13</b> Mixed Models</a></li>
<li class="chapter" data-level="14" data-path="ordination.html"><a href="ordination.html"><i class="fa fa-check"></i><b>14</b> Ordination</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ordination.html"><a href="ordination.html#libraries-and-data"><i class="fa fa-check"></i><b>14.1</b> Libraries and Data</a></li>
<li class="chapter" data-level="14.2" data-path="ordination.html"><a href="ordination.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>14.2</b> Principal component analysis (PCA)</a></li>
<li class="chapter" data-level="14.3" data-path="ordination.html"><a href="ordination.html#pca-under-the-hood"><i class="fa fa-check"></i><b>14.3</b> PCA under the hood</a></li>
<li class="chapter" data-level="14.4" data-path="ordination.html"><a href="ordination.html#principal-components-as-explanatory-variables"><i class="fa fa-check"></i><b>14.4</b> Principal components as explanatory variables</a></li>
<li class="chapter" data-level="14.5" data-path="ordination.html"><a href="ordination.html#nmds"><i class="fa fa-check"></i><b>14.5</b> NMDS</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>15</b> Clustering</a>
<ul>
<li class="chapter" data-level="15.1" data-path="clustering.html"><a href="clustering.html#libraries-and-data-1"><i class="fa fa-check"></i><b>15.1</b> Libraries and Data</a></li>
<li class="chapter" data-level="15.2" data-path="clustering.html"><a href="clustering.html#distances"><i class="fa fa-check"></i><b>15.2</b> Distances</a></li>
<li class="chapter" data-level="15.3" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>15.3</b> K-means clustering</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html"><i class="fa fa-check"></i><b>16</b> REcoStats: Linear Models</a>
<ul>
<li class="chapter" data-level="16.1" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html#fitting-a-simple-linear-regression-1"><i class="fa fa-check"></i><b>16.1</b> Fitting a simple linear regression</a></li>
<li class="chapter" data-level="16.2" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html#interpreting-a-simple-linear-regression-1"><i class="fa fa-check"></i><b>16.2</b> Interpreting a simple linear regression</a></li>
<li class="chapter" data-level="16.3" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html#simulating-data-from-a-linear-model-1"><i class="fa fa-check"></i><b>16.3</b> Simulating data from a linear model</a></li>
<li class="chapter" data-level="16.4" data-path="recostats-linear-models.html"><a href="recostats-linear-models.html#hypothesis-testing-and-permutation-testing"><i class="fa fa-check"></i><b>16.4</b> Hypothesis testing and permutation testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis and Statistical Thinking: An R Workbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="properties-of-estimators-and-inference" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Properties of Estimators and Inference</h1>
<div id="properties-of-point-estimators" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Properties of point estimators</h2>
<p>In this exercise, we’ll draw many simulated samples from a known distribution with known parameters. We will then consider these as instances of real datasets, and estimate parameters of the original distributions using different estimators applied to the datasets. Our goal will be to analyze properties of the estimators, namely their bias and variance. Recall the definitions of these two properties:</p>
<p>Bias: <span class="math display">\[B(\hat\theta_n) =  E[\hat\theta_n(D) - \theta] \]</span></p>
<p>The bias reflects the average difference between our estimator and the true parameter.</p>
<p>Variance: <span class="math display">\[Var(\hat\theta_n) =  E[ (\hat\theta_n(D) - E[\hat\theta_n(D)] )^2] \]</span></p>
<p>The variance reflects the variation of the estimator around its own mean (regardless of what the true parameter is). It is also the square of the standard error.</p>
<p><strong>Note</strong>: in real life, we will generally not know the distribution from which our data comes from, or the value of the parameters of that distribution (otherwise we wouldn’t be performing statistical inference). When developing or applying a particular estimator to our data, it is thus important to test the properties of that estimator under a variety of simulations and parameter choices. These simulations should aim to, as much as possible, mimic the possible range of processes and parameters that could be generating our data.</p>
<p>We will first create a function that draws a user-specified number (<code>nsim</code>) of independent data samples of size <code>sampsize</code> from a distribution of choice. By default this distribution is the Normal distribution (<code>rnorm</code>), but it can be changed by the user:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="properties-of-estimators-and-inference.html#cb1-1" aria-hidden="true" tabindex="-1"></a>simsamps <span class="ot">&lt;-</span> <span class="cf">function</span>(sampsize, <span class="at">nsim =</span> <span class="dv">10000</span>, <span class="at">rsim =</span> rnorm, ...){</span>
<span id="cb1-2"><a href="properties-of-estimators-and-inference.html#cb1-2" aria-hidden="true" tabindex="-1"></a>  sims <span class="ot">&lt;-</span> <span class="fu">rsim</span>(sampsize<span class="sc">*</span>nsim,...) <span class="co"># generate draws from a specified distribution</span></span>
<span id="cb1-3"><a href="properties-of-estimators-and-inference.html#cb1-3" aria-hidden="true" tabindex="-1"></a>  simmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">sample</span>(sims), <span class="at">nrow=</span>nsim, <span class="at">ncol=</span>sampsize) <span class="co"># organize those draws into a matrix</span></span>
<span id="cb1-4"><a href="properties-of-estimators-and-inference.html#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(simmat) <span class="co"># provide the matrix as output</span></span>
<span id="cb1-5"><a href="properties-of-estimators-and-inference.html#cb1-5" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The output of this function is a matrix, with row number equal to the number of generated simulations, and column number equal to the size of each simulation. For example, to create 10,000 simulations, each of size 30, from a normal distribution, we would write:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="properties-of-estimators-and-inference.html#cb2-1" aria-hidden="true" tabindex="-1"></a>testmat <span class="ot">&lt;-</span> <span class="fu">simsamps</span>(<span class="dv">30</span>,<span class="dv">10000</span>,rnorm) <span class="co"># generate matrix of 10,000 Normal data sets, each of size 30</span></span>
<span id="cb2-2"><a href="properties-of-estimators-and-inference.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(testmat) <span class="co"># dimensions of matrix</span></span></code></pre></div>
<pre><code>## [1] 10000    30</code></pre>
<p>What type of Normal distribution are we sampling from though? By default, the function rnorm samples from a standard Normal distribution with mean equal to 0 and standard deviation equal to 1. In our function <code>simsamps</code>, we are only feeding one argument to the normal distribution (the number of draws we want to obtain from it). The three dots placed in both the argument of the <code>simsamps</code> function and in its internal call to <code>rsim</code> allows us to feed more parameters to <code>rsim</code>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="properties-of-estimators-and-inference.html#cb4-1" aria-hidden="true" tabindex="-1"></a>testmat <span class="ot">&lt;-</span> <span class="fu">simsamps</span>(<span class="dv">30</span>,<span class="dv">100</span>,rnorm,<span class="at">mean=</span><span class="dv">3</span>,<span class="at">sd=</span><span class="dv">5</span>) <span class="co"># generate matrix of 1000 Normal(3,5) data sets of size 30</span></span></code></pre></div>
<p>We can obtain the sample mean and sample median from each of these datasets, using the <code>apply</code> function:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="properties-of-estimators-and-inference.html#cb5-1" aria-hidden="true" tabindex="-1"></a>testmean <span class="ot">&lt;-</span> <span class="fu">apply</span>(testmat,<span class="dv">1</span>,mean)</span>
<span id="cb5-2"><a href="properties-of-estimators-and-inference.html#cb5-2" aria-hidden="true" tabindex="-1"></a>testmedian <span class="ot">&lt;-</span> <span class="fu">apply</span>(testmat,<span class="dv">1</span>,median)</span></code></pre></div>
<p><strong>Exercise</strong>: Create 20,000 simulated data sets of size 5, each drawn from a Normal distribution with expected value equal to 4 and standard deviation equal to 10. Create a histogram of the sample means from all the data sets. Draw a blue line where the average of all these means is located, and a red line where the <strong>expected value</strong> of the original source distribution is located. Next, do the same for all the sample medians.</p>
<p>Now, try answering these questions: Is the sample mean a biased estimator of the expected value of a normal distribution? Is the sample median a biased estimator of the expected value of a normal distribution? Which estimator has the highest variance? Which estimator would you use if given a dataset like one of the ones we simulated, in order to estimate the expected value of this Normal distribution?</p>
<p><strong>Exercise</strong>: Create 20,000 simulated data sets of size 10, each drawn from an Exponential distribution with rate equal to 2. Create a histogram of the sample means from all the data sets. Draw a blue line where the average of all these means is located, and a red line where the <strong>expected value</strong> of the original source distribution is located. Recall that the expected value of this distribution is 1/rate = 0.5.</p>
<p>Now, try answering these questions: Is the sample mean a biased estimator of the expected value of an exponential distribution? Is the sample median a biased estimator of the expected value of an exponential distribution? Which of these two estimators would you use if you were trying to estimate the rate of an exponential process (e.g. the rate at which buses arrive at a station), from a dataset like one of the ones we just simulated?</p>
<p>The histograms we’ve built are examples of <strong>sampling distributions of estimators</strong>. They serve to visualize the spread of an estimator’s distribution around its own mean, and allow us to determine whether that estimator’s mean is equal to the expected value of our distribution of interest.</p>
</div>
<div id="obtaining-confidence-intervals" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Obtaining confidence intervals</h2>
<p>Confidence intervals (CIs) denote how sure we are about the value of a parameter. Importantly, CIs are statements made about an infinite number of data sets, which can be a bit counter-intuitive. For example, let’s imagine a parameter of interest called <span class="math inline">\(\theta\)</span>, which has a value that we don’t know. If we had an infinite number of data sets (<span class="math inline">\(i=1,2,3,...,\infty\)</span>), the lower and upper 95% confidence intervals for each dataset <span class="math inline">\(i\)</span> are the two values that should bound (contain) the unknown parameter <span class="math inline">\(\theta\)</span> in 95% of those datasets. Each data set will have its own confidence interval, but we can be sure that that interval will contain the unknown parameter 95% of the time.</p>
<p>Generally, we only have <strong>one set of data points</strong>, so this statement might sound a bit confusing. How can we make statements about infinite data sets, when we only have one set? In practice, the confidence interval is an approximation based on the <strong>spread (variance) of an estimator’s (<span class="math inline">\(\hat{\theta}\)</span>) sampling distribution around the expected value of the unknown parameter (<span class="math inline">\(\theta\)</span>)</strong>.
If a given estimator <span class="math inline">\(\hat{\theta}\)</span> of a data set of size n:</p>
<ol style="list-style-type: decimal">
<li>has a known standard error:</li>
</ol>
<p><span class="math display">\[\omega = SE(\hat{\theta}_n) = \sqrt{Var(\hat{\theta}_n)}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>is unbiased:</li>
</ol>
<p><span class="math display">\[E[\hat{\theta}_n - \theta] = 0\]</span></p>
<p>and</p>
<ol start="3" style="list-style-type: decimal">
<li>has a normal sampling distribution:</li>
</ol>
<p><span class="math display">\[\hat{\theta}_n \sim Normal\]</span></p>
<p>then the confidence intervals that will contain the true value of the parameter 95% of the time are:</p>
<p><span class="math display">\[(\hat{\theta} + \omega\  q_{2.5\%}, \hat{\theta} + \omega\ q_{97.5\%})\]</span>
Here, <span class="math inline">\(q_{x\%}\)</span> is the <span class="math inline">\(x\%\)</span> quantile function of a standard Normal(0,1) distribution. This value marks a limit such that <span class="math inline">\(x\%\)</span> of the probability mass of a distribution is to the left (lower than the value), and <span class="math inline">\(1-x\%\)</span> is to the right (higher than the value).</p>
<p>Because the standard normal(0,1) distribution is symmetric and centered around 0, <span class="math inline">\(q_{2.5\%} = -q_{97.5\%}\)</span>, so we can also write the CI as:</p>
<p><span class="math display">\[(\hat{\theta} - \omega\  q_{97.5\%}, \hat{\theta} + \omega\ q_{97.5\%})\]</span></p>
<p>If we compute these boundaries for a particular data set we study, and the estimator we’re applying satisfies the 2 conditions above, we can be sure that these boundaries will contain the true parameter <span class="math inline">\(95\%\)</span> of the time (out of an infinite number of possible data sets that we could have obtained, in theory, from the phenomenon of interest).</p>
<p>Note that some of the sampling distributions we saw in the previous section are neither normal nor unbiased. For example, the sampling distribution of the mean of a data set that is exponentially distributed is not normal, and the distribution of the median of such data set is neither normal nor unbiased! Thus, it would be inappropriate to compute the 95% CIs using the equation above in those cases. Later on in this class, we’ll figure out ways to obtain more general CIs that do not depend on the strict assumptions of unbiasedness and normality of the estimator’s sampling distribution.</p>
<p>The mean of a normally distributed dataset is both unbiased and normally distributed, so those assumptions hold in that case. Let’s verify that:</p>
<p><strong>Exercise</strong>: Create 20,000 simulated data sets of size 5, each drawn from a Normal distribution with expected value equal to 4 and standard deviation equal to 10. For each dataset, compute the sample mean. Then, compute the standard error of the mean (standard deviation over all means). Finally, use the standard error to compute the <span class="math inline">\(95\%\)</span> confidence intervals for each data set, using the formula stated above. How often do these confidence intervals contain the expected value? Hint: to obtain the quantiles of a standard Normal(x) distribution, you can use the function <code>qnorm</code>. For example, the <span class="math inline">\(35\%\)</span> quantile of a standard Normal(0,1) distribution is equal to <code>qnorm(0.35,mean=0,sd=1)</code>.</p>
<p>Because we are using simulations, we can obtain the standard error of the mean by generating many data sets. In practice, when working with a single data set, the standard error <span class="math inline">\(\omega\)</span> is generally unknown. However, it can be approximated using the sample standard deviation <span class="math inline">\(s\)</span> of the dataset we are studying (using the function <code>sd()</code>):</p>
<p><span class="math display">\[(\hat{\theta} + \frac{s}{\sqrt{n}}q_{2.5\%}, \hat{\theta} + \frac{s}{\sqrt{n}}q_{97.5\%})\]</span></p>
<p><strong>Exercise</strong>: Repeat the exercise above, but instead of using the standard deviation of each data set to approximate the standard error, compute the standard error of the means across all datasete, then plug that standard error into the formula above for obtaining confidence intervals. How often do these confidence intervals contain the expected value? Repeat this exercise multiple times. Is the proportion of bounded means as accurate as in the previous exercise?</p>
<p>As you’ve probably noticed, this will lead to overly confident boundaries, because we’ve replaced the standard error with a rough approximation to it. To correct for this, we must replace the quantiles of the Normal distribution with the quantiles of a distribution with slightly bigger tails: a ‘more uncertain’ distribution, called the t-distribution. This serves to correct for the extra uncertainty that we are bringing in by using the sample standard deviation <span class="math inline">\(s\)</span>:</p>
<p><img src="DataScience_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>The t-distribution has a single parameter (called the ‘degrees of freedom’), and represents how much information we have about the shape of the sampling distribution. In our case, this parameter should be set to <span class="math inline">\(n-1\)</span> where <span class="math inline">\(n\)</span> is the size of our data set. The larger our dataset, the bigger this parameter, and the closer the t-distribution will become to a Normal distribution.</p>
<p>Thus, to compute the <span class="math inline">\(95\%\)</span> confidence intervals when working with a real data set that we know is normally distributed, we can use this formula:</p>
<p><span class="math display">\[(\hat{\theta} + \frac{s}{\sqrt{n}}t_{2.5\%}, \hat{\theta} + \frac{s}{\sqrt{n}}t_{97.5\%})\]</span>
where <span class="math inline">\(t_{x\%}\)</span> is the <span class="math inline">\(x\%\)</span> quantile function of the t-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</p>
<p><strong>Exercise</strong>: Repeat the exercise above, but instead of using the standard normal(0,1) quantiles, use the quantiles from a t-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom, where <span class="math inline">\(n\)</span> is the size of each data set. Hint: You can obtain the quantile of a t-distribution using the function <code>qt()</code>. For example, the <span class="math inline">\(30\%\)</span> quantile of a t-distribution with 4 degrees of freedom is equal to <code>qt(0.3,4)</code>.</p>
</div>
<div id="hypothesis-testing-1" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Hypothesis testing</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="frequentist-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DataScience.pdf", "DataScience.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
